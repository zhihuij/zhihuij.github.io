{"posts":[{"title":"ELF文件的加载和动态链接过程","text":"今天看到有人写一个深入Hello World的文章，想起来读研的时候做的一个关于程序加载和链接的课程设计，也是以Hello World为例说明的，随发出来共享。 背景本文的目的：大家对于Hello World程序应该非常熟悉，随便使用哪一种语言，即使还不熟悉的语言，写出一个Hello World程序应该毫不费力，但是如果让大家详细的说明这个程序加载和链接的过程，以及后续的符号动态解析过程，可能还会有点困难。本文就是以一个最基本的C语言版本Hello World程序为基础，了解Linux下ELF文件的格式，分析并验证ELF文件和加载和动态链接的具有实现。 123456789/* hello.c */#include &lt;stdio.h&gt;int main(){ printf(“hello world!\\n”); return 0;}$ gcc –o hello hello.c 本文的实验平台 Ubuntu 7.04 Linux kernel 2.6.20 gcc 4.1.2 glibc 2.5 gdb 6.6 objdump/readelf 2.17.50 本文的组织 第一部分大致描述ELF文件的格式； 第二部分分析ELF文件在内核空间的加载过程； 第三部分分析ELF文件在运行过程中符号的动态解析过程；（以上各部分都是以Hello World程序为例说明） 第四部分简要总结； 第五部分阐明需要深入了解的东西。 ELF文件格式概述Executable and Linking Format(ELF)文件是x86 Linux系统下的一种常用目标文件(object file)格式，有三种主要类型: 适于连接的可重定位文件(relocatable file)，可与其它目标文件一起创建可执行文件和共享目标文件。 适于执行的可执行文件(executable file)，用于提供程序的进程映像，加载的内存执行。 共享目标文件(shared object file)，连接器可将它与其它可重定位文件和共享目标文件连接成其它的目标文件，动态连接器又可将它与可执行文件和其它共享目标文件结合起来创建一个进程映像。 ELF文件格式比较复杂，本文只是简要介绍它的结构，希望能给想了解ELF文件结构的读者以帮助。具体详尽的资料请参阅专门的ELF文档。 文件格式为了方便和高效，ELF文件内容有两个平行的视角:一个是程序连接角度，另一个是程序运行角度，如图所示。 ELF header在文件开始处描述了整个文件的组织，Section提供了目标文件的各项信息（如指令、数据、符号表、重定位信息等），Program header table指出怎样创建进程映像，含有每个program header的入口，section header table包含每一个section的入口，给出名字、大小等信息。 数据表示ELF数据编码顺序与机器相关，数据类型有六种，见下表： ELF文件头像bmp、exe等文件一样，ELF的文件头包含整个文件的控制结构。它的定义如下： 190 #define EI_NIDENT 16191 192 typedef struct elf32_hdr{193 unsigned char e_ident[EI_NIDENT]; 194 Elf32_Half e_type; /* file type */195 Elf32_Half e_machine; /* architecture */196 Elf32_Word e_version;197 Elf32_Addr e_entry; /* entry point */198 Elf32_Off e_phoff; /* PH table offset */199 Elf32_Off e_shoff; /* SH table offset */200 Elf32_Word e_flags;201 Elf32_Half e_ehsize; /* ELF header size in bytes */202 Elf32_Half e_phentsize; /* PH size */203 Elf32_Half e_phnum; /* PH number */204 Elf32_Half e_shentsize; /* SH size */205 Elf32_Half e_shnum; /* SH number */206 Elf32_Half e_shstrndx; /* SH name string table index */207 } Elf32_Ehdr; 其中E_ident的16个字节标明是个ELF文件（7F+’E’+’L’+’F’）。e_type表示文件类型，2表示可执行文件。e_machine说明机器类别，3表示386机器，8表示MIPS机器。e_entry给出进程开始的虚地址，即系统将控制转移的位置。e_phoff指出program header table的文件偏移，e_phentsize表示一个program header表中的入口的长度（字节数表示），e_phnum给出program header表中的入口数目。类似的，e_shoff，e_shentsize，e_shnum 分别表示section header表的文件偏移，表中每个入口的的字节数和入口数目。e_flags给出与处理器相关的标志，e_ehsize给出ELF文件头的长度（字节数表示）。e_shstrndx表示section名表的位置，指出在section header表中的索引。 Section Header目标文件的section header table可以定位所有的section，它是一个Elf32_Shdr结构的数组，Section头表的索引是这个数组的下标。有些索引号是保留的，目标文件不能使用这些特殊的索引。 Section包含目标文件除了ELF文件头、程序头表、section头表的所有信息，而且目标文件section满足几个条件： 目标文件中的每个section都只有一个section头项描述，可以存在不指示任何section的section头项。 每个section在文件中占据一块连续的空间。 Section之间不可重叠。 目标文件可以有非活动空间，各种headers和sections没有覆盖目标文件的每一个字节，这些非活动空间是没有定义的。 Section header结构定义如下： 288 typedef struct {289 Elf32_Word sh_name; /* name of section, index */290 Elf32_Word sh_type; 291 Elf32_Word sh_flags;292 Elf32_Addr sh_addr; /* memory address, if any */293 Elf32_Off sh_offset;294 Elf32_Word sh_size; /* section size in file */295 Elf32_Word sh_link;296 Elf32_Word sh_info;297 Elf32_Word sh_addralign;298 Elf32_Word sh_entsize; /* fixed entry size, if have */299 } Elf32_Shdr; 其中sh_name指出section的名字，它的值是后面将会讲到的section header string table中的索引，指出一个以null结尾的字符串。sh_type是类别，sh_flags指示该section在进程执行时的特性。sh_addr指出若此section在进程的内存映像中出现，则给出开始的虚地址。sh_offset给出此section在文件中的偏移。其它字段的意义不太常用，在此不细述。 文件的section含有程序和控制信息，系统使用一些特定的section，并有其固定的类型和属性（由sh_type和sh_info指出）。下面介绍几个常用到的section:“.bss”段含有占据程序内存映像的未初始化数据，当程序开始运行时系统对这段数据初始为零，但这个section并不占文件空间。“.data.”和“.data1”段包含占据内存映像的初始化数据。“.rodata”和“.rodata1”段含程序映像中的只读数据。“.shstrtab”段含有每个section的名字，由section入口结构中的sh_name索引。“.strtab”段含有表示符号表(symbol table)名字的字符串。“.symtab”段含有文件的符号表，在后文专门介绍。“.text”段包含程序的可执行指令。 当然一个实际的ELF文件中，会包含很多的section，如.got，.plt等等，我们这里就不一一细述了，需要时再详细的说明。 Program Header目标文件或者共享文件的program header table描述了系统执行一个程序所需要的段或者其它信息。目标文件的一个段（segment）包含一个或者多个section。Program header只对可执行文件和共享目标文件有意义，对于程序的链接没有任何意义。结构定义如下： 232 typedef struct elf32_phdr{233 Elf32_Word p_type; 234 Elf32_Off p_offset;235 Elf32_Addr p_vaddr; /* virtual address */236 Elf32_Addr p_paddr; /* ignore */237 Elf32_Word p_filesz; /* segment size in file */238 Elf32_Word p_memsz; /* size in memory */239 Elf32_Word p_flags;240 Elf32_Word p_align; 241 } Elf32_Phdr; 其中p_type描述段的类型；p_offset给出该段相对于文件开关的偏移量；p_vaddr给出该段所在的虚拟地址；p_paddr给出该段的物理地址，在Linux x86内核中，这项并没有被使用；p_filesz给出该段的大小，在字节为单元，可能为0；p_memsz给出该段在内存中所占的大小，可能为0；p_filesze与p_memsz的值可能会不相等。 Symbol Table目标文件的符号表包含定位或重定位程序符号定义和引用时所需要的信息。符号表入口结构定义如下： 171 typedef struct elf32_sym{172 Elf32_Word st_name;173 Elf32_Addr st_value;174 Elf32_Word st_size;175 unsigned char st_info;176 unsigned char st_other;177 Elf32_Half st_shndx;178 } Elf32_Sym; 其中st_name包含指向符号表字符串表(strtab)中的索引，从而可以获得符号名。st_value指出符号的值，可能是一个绝对值、地址等。st_size指出符号相关的内存大小，比如一个数据结构包含的字节数等。st_info规定了符号的类型和绑定属性，指出这个符号是一个数据名、函数名、section名还是源文件名；并且指出该符号的绑定属性是local、global还是weak。 Section和Segment的区别和联系可执行文件中，一个program header描述的内容称为一个段（segment）。Segment包含一个或者多个section，我们以Hello World程序为例，看一下section与segment的映射关系： 如上图红色区域所示，就是我们经常提到的文本段和数据段，由图中绿色部分的映射关系可知，文本段并不仅仅包含.text节，数据段也不仅仅包含.data节，而是都包含了多个section。 ELF文件的加载过程加载和动态链接的简要介绍从编译/链接和运行的角度看，应用程序和库程序的连接有两种方式。一种是固定的、静态的连接，就是把需要用到的库函数的目标代码（二进制）代码从程序库中抽取出来，链接进应用软件的目标映像中；另一种是动态链接，是指库函数的代码并不进入应用软件的目标映像，应用软件在编译/链接阶段并不完成跟库函数的链接，而是把函数库的映像也交给用户，到启动应用软件目标映像运行时才把程序库的映像也装入用户空间（并加以定位），再完成应用软件与库函数的连接。 这样，就有了两种不同的ELF格式映像。一种是静态链接的，在装入/启动其运行时无需装入函数库映像、也无需进行动态连接。另一种是动态连接，需要在装入/启动其运行时同时装入函数库映像并进行动态链接。Linux内核既支持静态链接的ELF映像，也支持动态链接的ELF映像，而且装入/启动ELF映像必需由内核完成，而动态连接的实现则既可以在内核中完成，也可在用户空间完成。因此，GNU把对于动态链接ELF映像的支持作了分工：把ELF映像的装入/启动入在Linux内核中；而把动态链接的实现放在用户空间（glibc），并为此提供一个称为“解释器”（ld-linux.so.2）的工具软件，而解释器的装入/启动也由内核负责，这在后面我们分析ELF文件的加载时就可以看到。 这部分主要说明ELF文件在内核空间的加载过程，下一部分对用户空间符号的动态解析过程进行说明。 Linux可执行文件类型的注册机制在说明ELF文件的加载过程以前，我们先回答一个问题，就是：为什么Linux可以运行ELF文件？回答：内核对所支持的每种可执行的程序类型都有个struct linux_binfmt的数据结构，定义如下： 53 /*54 * This structure defines the functions that are used to load the binary formats that55 * linux accepts.56 */57 struct linux_binfmt {58 struct linux_binfmt * next;59 struct module *module;60 int (*load_binary)(struct linux_binprm *, struct pt_regs * regs);61 int (*load_shlib)(struct file *)62 int (*core_dump)(long signr, struct pt_regs * regs, struct file * file);63 unsigned long min_coredump; /* minimal dump size */64 int hasvdso;65 }; 其中的load_binary函数指针指向的就是一个可执行程序的处理函数。而我们研究的ELF文件格式的定义如下： 74 static struct linux_binfmt elf_format = {75 .module = THIS_MODULE,76 .load_binary = load_elf_binary,77 .load_shlib = load_elf_library,78 .core_dump = elf_core_dump,79 .min_coredump = ELF_EXEC_PAGESIZE,80 .hasvdso = 181 }; 要支持ELF文件的运行，则必须向内核登记这个数据结构，加入到内核支持的可执行程序的队列中。内核提供两个函数来完成这个功能，一个注册，一个注销，即： 72 int register_binfmt(struct linux_binfmt * fmt)96 int unregister_binfmt(struct linux_binfmt * fmt) 当需要运行一个程序时，则扫描这个队列，让各个数据结构所提供的处理程序，ELF中即为load_elf_binary，逐一前来认领，如果某个格式的处理程序发现相符后，便执行该格式映像的装入和启动。 内核空间的加载过程内核中实际执行execv()或execve()系统调用的程序是do_execve()，这个函数先打开目标映像文件，并从目标文件的头部（第一个字节开始）读入若干（当前Linux内核中是128）字节（实际上就是填充ELF文件头，下面的分析可以看到），然后调用另一个函数search_binary_handler()，在此函数里面，它会搜索我们上面提到的Linux支持的可执行文件类型队列，让各种可执行程序的处理程序前来认领和处理。如果类型匹配，则调用load_binary函数指针所指向的处理函数来处理目标映像文件。在ELF文件格式中，处理函数是load_elf_binary函数，下面主要就是分析load_elf_binary函数的执行过程（说明：因为内核中实际的加载需要涉及到很多东西，这里只关注跟ELF文件的处理相关的代码）： 550 struct {551 struct elfhdr elf_ex;552 struct elfhdr interp_elf_ex;553 struct exec interp_ex;554 } *loc;556 loc = kmalloc(sizeof(*loc), GFP_KERNEL);562 /* Get the exec-header */563 loc-&gt;elf_ex = *((struct elfhdr *)bprm-&gt;buf); ……566 /* First of all, some simple consistency checks */567 if (memcmp(loc-&gt;elf_ex.e_ident, ELFMAG, SELFMAG) != 0)568 goto out;570 if (loc-&gt;elf_ex.e_type != ET_EXEC &amp;&amp; loc-&gt;elf_ex.e_type != ET_DYN)571 goto out; 在load_elf_binary之前，内核已经使用映像文件的前128个字节对bprm-&gt;buf进行了填充，563行就是使用这此信息填充映像的文件头（具体数据结构定义见第一部分，ELF文件头节），然后567行就是比较文件头的前四个字节，查看是否是ELF文件类型定义的“\\177ELF”。除这4个字符以外，还要看映像的类型是否ET_EXEC和ET_DYN之一；前者表示可执行映像，后者表示共享库。 577 /* Now read in all of the header information */580 if (loc-&gt;elf_ex.e_phnum &lt; 1 ||581 loc-&gt;elf_ex.e_phnum &gt; 65536U / sizeof(struct elf_phdr))582 goto out;583 size = loc-&gt;elf_ex.e_phnum * sizeof(struct elf_phdr); ……585 elf_phdata = kmalloc(size, GFP_KERNEL); ……589 retval = kernel_read(bprm-&gt;file, loc-&gt;elf_ex.e_phoff,590 (char *)elf_phdata, size); 这块就是通过kernel_read读入整个program header table。从代码中可以看到，一个可执行程序必须至少有一个段（segment），而所有段的大小之和不能超过64K。 614 elf_ppnt = elf_phdata; ……623 for (i = 0; i &lt; loc-&gt;elf_ex.e_phnum; i++) {624 if (elf_ppnt-&gt;p_type == PT_INTERP) { ……635 elf_interpreter = kmalloc(elf_ppnt-&gt;p_filesz, GFP_KERNEL); ……640 retval = kernel_read(bprm-&gt;file, elf_ppnt-&gt;p_offset,641 elf_interpreter,642 elf_ppnt-&gt;p_filesz); ……682 interpreter = open_exec(elf_interpreter); ……695 retval = kernel_read(interpreter, 0, bprm-&gt;buf,696 BINPRM_BUF_SIZE); ……703 /* Get the exec headers */ ……705 loc-&gt;interp_elf_ex = *((struct elfhdr *)bprm-&gt;buf);706 break;707 }708 elf_ppnt++;709 } 这个for循环的目的在于寻找和处理目标映像的“解释器”段。“解释器”段的类型为PT_INTERP，找到后就根据其位置的p_offset和大小p_filesz把整个“解释器”段的内容读入缓冲区（640-640）。这个“解释器”段实际上只是一个字符串，即解释器的文件名，如“/lib/ld-linux.so.2”。有了解释器的文件名以后，就通过open_exec()打开这个文件，再通过kernel_read()读入其开关128个字节（695~696），即解释器映像的头部。我们以Hello World程序为例，看一下这段中具体的内容： 其实从readelf程序的输出中，我们就可以看到需要解释器/lib/ld-linux.so.2，为了进一步的验证，我们用hd命令以16进制格式查看下类型为INTERP的段所在位置的内容，在上面的各个域可以看到，它位于偏移量为0x000114的位置，文件内占19个字节： 从上面红色部分可以看到，这个段中实际保存的就是“/lib/ld-linux.so.2”这个字符串。 814 for(i = 0, elf_ppnt = elf_phdata;815 i &lt; loc-&gt;elf_ex.e_phnum; i++, elf_ppnt++) { …… 819 if (elf_ppnt-&gt;p_type != PT_LOAD)820 continue; …… 870 error = elf_map(bprm-&gt;file, load_bias + vaddr, elf_ppnt,871 elf_prot, elf_flags); ……920 } 这段代码从目标映像的程序头中搜索类型为PT_LOAD的段（Segment）。在二进制映像中，只有类型为PT_LOAD的段才是需要装入的。当然在装入之前，需要确定装入的地址，只要考虑的就是页面对齐，还有该段的p_vaddr域的值（上面省略这部分内容）。确定了装入地址后，就通过elf_map()建立用户空间虚拟地址空间与目标映像文件中某个连续区间之间的映射，其返回值就是实际映射的起始地址。 946 if (elf_interpreter) { ……951 elf_entry = load_elf_interp(&amp;loc-&gt;interp_elf_ex,952 interpreter,953 &amp;interp_load_addr); ……965 } else {966 elf_entry = loc-&gt;elf_ex.e_entry; ……972 } 这段程序的逻辑非常简单：如果需要装入解释器，就通过load_elf_interp装入其映像（951~953），并把将来进入用户空间的入口地址设置成load_elf_interp()的返回值，即解释器映像的入口地址。而若不装入解释器，那么这个入口地址就是目标映像本身的入口地址。 991 create_elf_tables(bprm, &amp;loc-&gt;elf_ex,992 (interpreter_type == INTERPRETER_AOUT),993 load_addr, interp_load_addr); ……1028 start_thread(regs, elf_entry, bprm-&gt;p); 在完成装入，启动用户空间的映像运行之前，还需要为目标映像和解释器准备好一些有关的信息，这些信息包括常规的argc、envc等等，还有一些“辅助向量（Auxiliary Vector）”。这些信息需要复制到用户空间，使它们在CPU进入解释器或目标映像的程序入口时出现在用户空间堆栈上。这里的create_elf_tables()就起着这个作用。 最后，start_thread()这个宏操作会将eip和esp改成新的地址，就使得CPU在返回用户空间时就进入新的程序入口。如果存在解释器映像，那么这就是解释器映像的程序入口，否则就是目标映像的程序入口。那么什么情况下有解释器映像存在，什么情况下没有呢？如果目标映像与各种库的链接是静态链接，因而无需依靠共享库、即动态链接库，那就不需要解释器映像；否则就一定要有解释器映像存在。 以我们的Hello World为例，gcc在编译时，除非显示的使用static标签，否则所有程序的链接都是动态链接的，也就是说需要解释器。由此可见，我们的Hello World程序在被内核加载到内存，内核跳到用户空间后并不是执行Hello World的，而是先把控制权交到用户空间的解释器，由解释器加载运行用户程序所需要的动态库（Hello World需要libc），然后控制权才会转移到用户程序。 ELF文件中符号的动态解析过程上面一节提到，控制权是先交到解释器，由解释器加载动态库，然后控制权才会到用户程序。因为时间原因，动态库的具体加载过程，并没有进行深入分析。大致的过程就是将每一个依赖的动态库都加载到内存，并形成一个链表，后面的符号解析过程主要就是在这个链表中搜索符号的定义。 我们后面主要就是以Hello World为例，分析程序是如何调用printf的： 查看一下gcc编译生成的Hello World程序的汇编代码（main函数部分）： 08048374 &lt;main&gt;: 8048374: 8d 4c 24 04 lea 0x4(%esp),%ecx …… 8048385: c7 04 24 6c 84 04 08 movl $0x804846c,(%esp) 804838c: e8 2b ff ff ff call 80482bc &lt;puts@plt&gt; 8048391: b8 00 00 00 00 mov $0x0,%eax 从上面的代码可以看出，经过编译后，printf函数的调用已经换成了puts函数（原因读者可以想一下）。其中的call指令就是调用puts函数。但从上面的代码可以看出，它调用的是puts@plt这个标号，它代表什么意思呢？在进一步说明符号的动态解析过程以前，需要先了解两个概念，一个是global offset table，一个是procedure linkage table。 Global Offset Table（GOT）在位置无关代码中，一般不能包含绝对虚拟地址（如共享库）。当在程序中引用某个共享库中的符号时，编译链接阶段并不知道这个符号的具体位置，只有等到动态链接器将所需要的共享库加载时进内存后，也就是在运行阶段，符号的地址才会最终确定。因此，需要有一个数据结构来保存符号的绝对地址，这就是GOT表的作用，GOT表中每项保存程序中引用其它符号的绝对地址。这样，程序就可以通过引用GOT表来获得某个符号的地址。 在x86结构中，GOT表的前三项保留，用于保存特殊的数据结构地址，其它的各项保存符号的绝对地址。对于符号的动态解析过程，我们只需要了解的就是第二项和第三项，即GOT[1]和GOT[2]：GOT[1]保存的是一个地址，指向已经加载的共享库的链表地址（前面提到加载的共享库会形成一个链表）；GOT[2]保存的是一个函数的地址，定义如下：GOT[2] = &amp;_dl_runtime_resolve，这个函数的主要作用就是找到某个符号的地址，并把它写到与此符号相关的GOT项中，然后将控制转移到目标函数，后面我们会详细分析。 Procedure Linkage Table（PLT）过程链接表（PLT）的作用就是将位置无关的函数调用转移到绝对地址。在编译链接时，链接器并不能控制执行从一个可执行文件或者共享文件中转移到另一个中（如前所说，这时候函数的地址还不能确定），因此，链接器将控制转移到PLT中的某一项。而PLT通过引用GOT表中的函数的绝对地址，来把控制转移到实际的函数。在实际的可执行程序或者共享目标文件中，GOT表在名称为.got.plt的section中，PLT表在名称为.plt的section中。大致的了解了GOT和PLT的内容后，我们查看一下puts@plt中到底是什么内容： Disassembly of section .plt:0804828c &lt;__gmon_start__@plt-0x10&gt;: 804828c: ff 35 68 95 04 08 pushl 0x8049568 8048292: ff 25 6c 95 04 08 jmp *0x804956c 8048298: 00 00 ......0804829c &lt;__gmon_start__@plt&gt;: 804829c: ff 25 70 95 04 08 jmp *0x8049570 80482a2: 68 00 00 00 00 push $0x0 80482a7: e9 e0 ff ff ff jmp 804828c &lt;_init+0x18&gt;080482ac &lt;__libc_start_main@plt&gt;: 80482ac: ff 25 74 95 04 08 jmp *0x8049574 80482b2: 68 08 00 00 00 push $0x8 80482b7: e9 d0 ff ff ff jmp 804828c &lt;_init+0x18&gt;080482bc &lt;puts@plt&gt;: 80482bc: ff 25 78 95 04 08 jmp *0x8049578 80482c2: 68 10 00 00 00 push $0x10 80482c7: e9 c0 ff ff ff jmp 804828c &lt;_init+0x18&gt; 可以看到puts@plt包含三条指令，程序中所有对有puts函数的调用都要先来到这里（Hello World里只有一次）。可以看出，除PLT0以外（就是__gmon_start__@plt-0x10所标记的内容），其它的所有PLT项的形式都是一样的，而且最后的jmp指令都是0x804828c，即PLT0为目标的。所不同的只是第一条jmp指令的目标和push指令中的数据。PLT0则与之不同，但是包括PLT0在内的每个表项都占16个字节，所以整个PLT就像个数组（实际是代码段）。另外，每个PLT表项中的第一条jmp指令是间接寻址的。比如我们的puts函数是以地址0x8049578处的内容为目标地址进行中跳转的。2. 顺着这个地址，我们进一步查看此处的内容： (gdb) x/w 0x80495780x8049578 &lt;_GLOBAL_OFFSET_TABLE_+20&gt;: 0x080482c2 从上面可以看出，这个地址就是GOT表中的一项。它里面的内容是0x80482c2，即puts@plt中的第二条指令。前面我们不是提到过，GOT中这里本应该是puts函数的地址才对，那为什么会这样呢？原来链接器在把所需要的共享库加载进内存后，并没有把共享库中的函数的地址写到GOT表项中，而是延迟到函数的第一次调用时，才会对函数的地址进行定位。 puts@plt的第二条指令是pushl $0x10，那这个0x10代表什么呢？ Relocation section '.rel.plt' at offset 0x25c contains 3 entries: Offset Info Type Sym.Value Sym. Name08049570 00000107 R_386_JUMP_SLOT 00000000 __gmon_start__08049574 00000207 R_386_JUMP_SLOT 00000000 __libc_start_main08049578 00000307 R_386_JUMP_SLOT 00000000 puts 其中的第三项就是puts函数的重定向信息，0x10即代表相对于.rel.plt这个section的偏移位置（每一项占8个字节）。其中的Offset这个域就代表的是puts函数地址在GOT表项中的位置，从上面puts@plt的第一条指令也可以验证这一点。向堆栈中压入这个偏移量的主要作用就是为了找到puts函数的符号名（即上面的Sym.Name域的“puts”这个字符串）以及puts函数地址在GOT表项中所占的位置，以便在函数定位完成后将函数的实际地址写到这个位置。4. puts@plt的第三条指令就跳到了PLT0的位置。这条指令只是将0x8049568这个数值压入堆栈，它实际上是GOT表项的第二个元素，即GOT[1]（共享库链表的地址）。5. 随即PLT0的第二条指令即跳到了GOT[2]中所保存的地址（间接寻址），即_dl_runtime_resolve这个函数的入口。6. _dl_runtime_resolve的定义如下： _dl_runtime_resolve: pushl %eax # Preserve registers otherwise clobbered. pushl %ecx pushl %edx movl 16(%esp), %edx # Copy args pushed by PLT in register. Note movl 12(%esp), %eax # that `fixup' takes its parameters in regs. call _dl_fixup # Call resolver. popl %edx # Get register content back. popl %ecx xchgl %eax, (%esp) # Get %eax contents end store function address. ret $8 # Jump to function address. 从调用puts函数到现在，总共有两次压栈操作，一次是压入puts函数的重定向信息的偏移量，一次是GOT[1]（共享库链表的地址）。上面的两次movl操作就是将这两个数据分别取到edx和eax，然后调用_dl_fixup（从寄存器取参数），此函数完成的功能就是找到puts函数的实际加载地址，并将它写到GOT中，然后通过eax将此值返回给_dl_runtime_resolve。xchagl这条指令，不仅将eax的值恢复，而且将puts函数的值压到栈顶，这样当执行ret指令后，控制就转移到puts函数内部。ret指令同时也完成了清栈动作，使栈顶为puts函数的返回地址（main函数中call指令的下一条指令），这样，当puts函数返回时，就返回到正确的位置。 当然，如果是第二次调用puts函数，那么就不需要这么复杂的过程，而只要通过GOT表中已经确定的函数地址直接进行跳转即可。下图是前面过程的一个示意图，红色为第一次函数调用的顺序，蓝色为后续函数调用的顺序（第1步都要执行）。 ELF文件加载和链接的简要总结 用户通过shell执行程序，shell通过exceve进入系统调用。（User-Mode） sys_execve经过一系列过程，并最终通过ELF文件的处理函数load_elf_binary将用户程序和ELF解释器加载进内存，并将控制权交给解释器。（Kernel-Mode） ELF解释器进行相关库的加载，并最终把控制权交给用户程序。由解释器处理用户程序运行过程中符号的动态解析。（User-Mode） 以后需要深入了解的内容 ELF如何映射进内存，以及内存的分配策略（可扩展到内存的管理）。 加载进内存的镜像，如何产生进程实体，并以怎样的方式进行调度队列，以及内核中的调度算法。 深入了解解释器加载共享库的实际过程，以及后续的符号查找过程。 内核模块的工作原理及实际的实现过程。","link":"/2012/03/09/elf-loading-and-linking/"},{"title":"中断线程的执行","text":"中断线程是指：线程正在运行，还没有正常退出（run方法顺利结束），而某个事件的发生导致该线程必须中断当前正在执行的任务，该线程或者退出，或者等待其它事件然后再继续执行。稳定的基于线程的服务，在程序退出时，必须能够安全的释放线程所占用的资源，减少对系统性能的影响。 Thread类提供的方法中与此功能相关的函数有Thread.stop，Thread.suspend，Thread.resume。但是这几个函数都不能安全的提供相应的功能：Thread.stop会导致对象处理不一致的状态，而Thread.suspend和Thread.resume则会导致出现死锁，具体可见API文档描述。 虽然Thread类没有一种直接又安全的机制中断线程的执行，但是却提供了一种协作机制来完成类似的功能：Thread.interrupt。但是，别误解，单纯调用这个函数并不能完成我们定义的中断线程的功能，很多时候，它只是在线程内部设置一个状态位，表示当前线程收到过interrupt请求。在解释Thread.interrupt之前，我们先看一下，如果没有相关机制的支持，我们自己怎样完成中断线程执行的功能（这里不区分Runnable和Thread；这个机制要正常工作，volatile关键字不能缺）： 12345678910class CancellableTask1 implements Runnable { private volatile boolean cancelled = false; @Override public void run() { while (!cancelled) { doSomething(); } } public void cancel() { cancelled = true; };} 现在考虑一个问题，如果doSomething中执行一个长时间阻塞的操作（比如sleep），那会发生什么情况？这个线程要么等待长时间（取决于阻塞操作等待的时间）后退出，要么一直不会退出（阻塞操作等待的事件没有发生）。这时候，Thread.interrupt就会发挥作用，来看一下它的API描述（原文比较详细，这里只是大致总结）： 如果当前线程处于阻塞状态（部分阻塞操作，一般情况下指可抛出InterruptedException的操作，如Thread.sleep），那么调用Thread.interrupt，该线程会收到一个InterruptedException，并且将当前线程的中断状态清除； 如果当前线程没有阻塞，那么调用Thread.interrupt后，当前线程的中断状态被设置成true。看描述Thread.interrupt是不能直接完成中断线程的目的，所以才说它是一种协作机制。我们来看一下这种协作机制在操作阻塞时完成中断线程的目标： 123456789101112131415class CancellableTask2 implements Runnable { @Override public void run() { try { while (true) { doSomething(); } } catch (InterruptedException e) { // Exit thread, or do something before exit. // Preserve interrupt status Thread.currentThread().interrupt(); } } public void cancel() { Thread.currentThread().interrupt(); } 这种方式下，如果有interrupt请求，线程会立即退出，当然，Thread.interrupt调用并没有强制线程一定要对interrupt请求作出响应，也可以忽略请求，继续运行（如CancellableTask3），这就取决于线程创建者采取的响应策略。只有清楚一个线程的响应策略时，才能利用Thread.interrupt机制来中断线程运行。 12345678910111213class CancellableTask3 implements Runnable { @Override public void run() { while (!Thread.currentThread().isInterrupted()) { try { doSomething(); } catch (InterruptedException e) { // Continue } } } public void cancel() { Thread.currentThread().interrupt(); }} 前面说明Thread.interrupt的作用时提到，只有部分阻塞操作会对interrupt请求作出响应抛出InterruptedException，那对interrupt请求无响应的操作，该怎么处理？如跟Socket读写相关的InputStream，OutputStream的read和write操作都不会interrupt请求作出响应，但是关闭底层的Socket是导致read和write操作招聘SocketException，所以也可以作为一种中断线程的方式。 JDK1.5后，Java中提供java.util.concurrent包，其中包含了若干跟并发和线程管理相关的功能。ThreadPoolExecutor.submit就可以通过返回一个Future来取消或中断当前任务的执行，Future底层的实现机制也是通过Thread.interrupt来实现的。Future.cancel只能中断对interrupt请求有响应的操作，如果阻塞的操作对interrupt请求无响应怎么办？那么可以通过重写ThreadPoolExecutor.newTaskFor（JDK1.6）来返回自定义的Future.cancel来实现。 当然， Thread.interrupt机制要实现类似Thread.suspend、Thread.resume提供的暂停和继续的语义可能比较麻烦，不过，JDK中提供了其它的一些方便的机制来完成这个目的，比如wait-and-notify（Object.wait和Object.notify）或者信号量等。","link":"/2011/11/21/interrupt-thread/"},{"title":"I&#x2F;O模型：阻塞、非阻塞 &amp; 同步、异步","text":"这篇文章主要总结下这几个概念，前几天看到微博里在讨论，当时也有点搞不清楚，昨天在看到Reactor和Proactor模式的时候，又提到相关概念，索性搞搞清楚，写个总结。 《Unix网络编程卷1：套接字联网API》（下面称为卷1）第6章对Unix I/O模型有5种划分：阻塞式I/O模型，非阻塞式I/O模型，I/O复用模型，信号驱动式I/O，异步I/O模型。这里我们只关心跟我们主题相关的四类： 阻塞式I/O模型：应用进程调用I/O操作时阻塞，只有等待要操作的数据准备好，并复制到应用进程的缓冲区中才返回。 非阻塞式I/O模型：当应用进程要调用的I/O操作会导致该进程进入阻塞状态时，该I/O调用返回一个错误，一般情况下，应用进程需要利用轮询的方式来检测某个操作是否就绪。数据就绪后，实际的I/O操作会等待数据复制到应用进程的缓冲区中以后才返回。 I/O复用模型：阻塞发生在select/poll的系统调用上，而不是阻塞在实际的I/O系统调用上。select/poll发现有数据就绪后，通过实际的I/O操作将数据复制到应用进程的缓冲区中。 异步I/O模型：应用进程通知内核开始一个异步I/O操作，并让内核在整个操作（包含将数据从内核复制到应该进程的缓冲区）完成后通知应用进程。 对于上面的分类，卷1给出了一个很形象的图，如下： 从上面图中可以看出，卷1中把I/O操作分为两个阶段，第一阶段等待数据可用，第二阶段将数据从内核复制到用户空间。前三种模型的区别在于第一阶段（阻塞式I/O阻塞在I/O操作上，非阻塞式I/O轮询，I/O复用阻塞在select/poll或者epoll上），第二阶段都是一样的，即这里的阻塞不阻塞体现在第一阶段，从这方面来说I/O复用类型也可以归类到阻塞式I/O，它与阻塞式I/O的区别在于阻塞的系统调用不同。而异步I/O的两个阶段都不会阻塞进程。 我们再来看看同步I/O与异步I/O（AIO），根据卷1的说明，同步I/O与异步I/O是由POSIX定义的两个术语： 同步I/O操作：实际的I/O操作将导致请求进程阻塞，直到I/O操作完成。 异步I/O操作：实际的I/O操作不导致请求进程阻塞。 由此定义来看，前面分类中的前三种：阻塞式I/O，非阻塞式I/O，I/O复用都属于同步I/O，因为第二阶段的数据复制都是阻塞的。而只有前面定义的异步I/O模型与这里的异步I/O操作吻合。 由异步I/O的定义来看，操作系统必须提供一种方式，在应用进程发出I/O操作后，可以在后台（而不是当前应用进程）完成数据等待和数据复制的工作，并最终通知应用进程I/O操作已经完成。 在Linux下有两种称为AIO的的接口。一个是由glibc提供，是由多线程来模拟：数据等待和数据复制的工作，由glibc创建线程来完成。数据复制完成后，执行I/O操作的线程通过回调函数的方式通知应用线程（严格来讲，这种方式不能算真正的AIO，因为用来执行实际I/O操作的线程还是阻塞在I/O操作上，只不过从应用进程的角度来看是异步方式的）。另一种是由内核提供的Kernel AIO，可以做到真正的内核异步通知（这种方式对读写方式，写入大小及偏移都有严格的要求），并且不支持网络I/O[1][2]，其实现原理本质上与下面要介绍的IOCP类似。 还有一种称为IOCP（Input/Output Completion Port）的AIO。从实现原理上讲，IOCP做完I/O操作后，将结果封装成完成包（completion packet）入队到完成端口的队列（FIFO）中去，应用线程从队列中读取到完成消息后，处理后续逻辑。从这方面来讲，IOCP类似生产者-消费者模型：生产者为内核，收到应用线程的I/O请求后，等待数据可用，并将结果数据复制到应用线程指定的缓冲区中后，然后入队一个完成消息；消费者为应用线程，一开始向内核提交I/O请求，并在队列上等待内核的完成消息（只不过，IOCP对同时可运行的消费者有限制），收到完成消息后，进行后续处理[3]。 从上面对Linux kernel AIO以及IOCP的介绍可以看出，这两种异步I/O操作的完成通知是通过入队消息到消息队列的方式来完成的，应用进程必须阻塞在消息队列上来等待完成消息（别被这里的阻塞混淆，AIO定义中的阻塞是指实际的I/O操作）。 Reactor与Proactor模式就分别对应同步I/O和异步I/O[4]：Reactor是在事件就绪时通知应用进程，应用进程需要完成实际的I/O操作；而Proactor是在I/O操作已经完成的时候（数据就绪，并且已经拷贝到应用进程的缓冲区中，实际的I/O操作由操作系统来完成）通知应用进程。 AIO在服务器设计方面很少被用到[5]，更多的使用在本地I/O方面[6]。 总结：其实要搞清楚这些概念，主要是搞清楚这些概念描述的主体是什么： 阻塞或者非阻塞I/O主要是指I/O操作第一阶段的完成方式，即数据还未准备好的时候，应用进程的表现，如果这里进程挂起，则为阻塞I/O，否则为非阻塞I/O。 同步或者异步I/O主要是指实际I/O操作的完成方式，同步意味着由应用进程发起并完成I/O操作，I/O操作未完成前，会导致应用进程挂起；异步意味着应用进程只发出I/O请求，并接收完成通知，实际I/O操作由系统完成，I/O操作进行时，应用进程可以继续工作。 参考资料： http://cnodejs.org/topic/4f16442ccae1f4aa270010a7/ http://lse.sourceforge.net/io/aio.html http://60.251.1.52/taiwan/technet/sysinternals/information/iocompletionports.mspx http://www.artima.com/articles/io_design_patterns2.html http://www.kegel.com/c10k.html#aio http://blog.yufeng.info/archives/741","link":"/2012/08/11/io-model/"},{"title":"Java并发源码分析 - 锁","text":"（注：文章里涉及到的代码分析，基于jdk1.7.0_10 Hotspot 64-Bit） 基本概念Java同步机制除了内置的synchronized（包含Object.wait/notify）以外，还通过concurrent包提供了多种锁，包含ReentrantLock、Semaphore、ReentrantReadWriteLock等，以及跟Object.wait/notify类似语义的Condition接口。 接口定义具体的接口（Lock，Condition）就不在这里赘述，只做个简单总结： Lock接口提供三种不同类型的获取锁接口：不响应中断（interrupt）、响应中断、可以设置超时； Condition接口提供类似Object.wait语义的四种await接口：不响应中断（interrupt）、响应中断、可以设置超时、可以设置deadline；不管哪一种await，都必须在调用前持有跟该Condition对象关联的锁，Condition的实现会保证await调用在进入阻塞状态前释放锁，并且在await调用返回时，重新持有锁。 锁类型 同synchronized一样，concurrent包里提供的锁都是可重入的（reentrant）：一个线程在持有一个锁时，在不释放该锁的前提下，可多次重新持有该锁； 互斥锁和共享锁：在一个线程持有锁的时候，如果其它线程不能再持有该锁，则为互斥锁，否则为共享锁；concurrent包里的ReentrantLock为互斥锁，Semaphore为共享锁，ReentrantReadWriteLock是共享锁及互斥锁的结合； 公平锁和非公平锁：公平锁保证线程以FIFO的顺序持有锁（不包含tryLock接口），但非公平锁不保证这点：在有线程在排队等待获取当前锁的时候，新的线程可以直接竞争成功并持有锁； 基本框架简单查看一下ReetrantLock、Semaphore等类的实现，会发现都依赖于AbstractQueuedSynchronizer（AQS）这个类，这个其实是concurrent包里实现同步机制的一个核心框架，可以通过这篇论文来了解这个框架。该框架的核心实现要素包含以下三点： 同步状态的原子性管理 等待队列的管理 线程的阻塞和唤醒 同步状态的原子性管理AQS将状态定义为一个整型变量（volatile int state），对它的修改AQS提供了两个接口，一个是基于volatile语义： 549550551protected final void setState(int newState) { state = newState;} 另外一个依赖于Unsafe.compareAndSwapInt： 564565566567protected final boolean compareAndSetState(int expect, int update) { // See below for intrinsics setup to support this return unsafe.compareAndSwapInt(this, stateOffset, expect, update);} 那什么时候用setState，什么时候用compareAndSetState呢？简单看了下调用关系，有如下特征： 初始化state时一般用setState，比如：Semaphore、CountDownLatch、ReentrantReadWriteLock等的AQS子类初始化； 互斥锁的可重入处理逻辑中一般调用setState，比如：ReentrantLock的tryAcquire，ReentrantReadWriteLock的tryAcquire； 互斥锁的释放锁操作一般调用setState，比如：ReentrantLock的tryRelease，ReentrantReadWriteLock的tryRelease； 其它情况下都调用compareAndSetState。 从以上的情况来看，应该是在基本无竞争（初始化，重入处理、互斥锁的释放）的情况下调用setState；竞争比较激烈的情况下调用compareAndSetState。 等待队列的管理AQS使用CLH队列的变种来管理等待线程，每个等待线程为一个结点（AbstractQueuedSynchronizer.Node），后文会混用结点和线程。 CLH队列中结点之间并不存在实际的连接，后继结点在等待锁的时候只是在前续结点的状态字段上自旋，直到获取锁。论文对AQS使用prev及next字段的解释是： prev主要为了完成超时及取消语义：如果前继结点取消，那么就是向前找到一个未取消的前继结点； next的主要作用在于优化后继结点的查找，避免每次都需要从tail结点向前反向查找。 线程的阻塞和唤醒依赖于LockSupport.park（阻塞当前线程，实际调用Unsafe.park）及LockSupport.unpark（唤醒指定线程，实际调用Unsafe.unpark）；根据LockSupport的Java doc可以了解到以下内容： park与unpark使用类似Semaphore的许可机制，如果当前线程拥有许可，那个park会消费掉该许可，并立即返回；如果当前线程没有许可，则当前线程会阻塞；unpark会导致指定线程的许可可用； 许可不会累加，最多只有一个，也就是说连续多次的unpark并不会导致许可变多，也就是说如下代码还是会导致当前线程阻塞： 1234LockSupport.unpark(Thread.currentThread()); LockSupport.unpark(Thread.currentThread()); LockSupport.park(); LockSupport.park(); 关于park()和park(Object blocker)的区别，Object blocker参数的作用在于允许记录当前线程被阻塞的原因，以便监控分析工具进行分析。官方的文档中也更建议使用park(Object blocker)。 AQS实现分析AQS之前先了解下concurrent包里的类是如何使用AQS的。AQS是抽象类，ReentrantLock、Semaphore等类会在使用时定义一个子类（Sync，一般还会根据是否是公平锁定义FireSync、NonfairSync），根据具体的需要重写AQS定义的四个protected接口： 1234567891011/** * 用于互斥锁。 */protected boolean tryAcquire(int arg);protected boolean tryRelease(int arg);/** * 用于共享锁。 */protected int tryAcquireShared(int arg);protected boolean tryReleaseShared(int arg); 注意返回值上，只有tryAcquireShared的返回值为int：大于0时，代表当前获取锁成功，后续的获取锁请求也可能会成功；等于0时，代表当前获取锁成功，后续获取锁请求必须等待；小于0时，代表当前获取锁失败，必须等待；其它返回值都为boolean，true则成功，false失败。 上述这几个接口的主要作用是什么呢？将管理锁（或者其它实现）的状态的任务交给具体实现类，这样AQS就不需要知道各个不同锁机制的状态之间的差别，从而简化AQS的实现。 然后具体的锁实现会调用AQS定义的几个公有方法来获取或者释放锁： 123456789101112131415/** * 用于互斥锁：分别对应不响应中断、响应中断、可设置超时的获取锁接口. */public final void acquire(int arg);public final void acquireInterruptibly(int arg) throws InterruptedException;public final boolean tryAcquireNanos(int arg, long nanosTimeout) throws InterruptedException;public final boolean release(int arg);/** * 用于共享锁：分别对应不响应中断、响应中断、可设置超时的获取锁接口. */public final void acquireShared(int arg);public final void acquireSharedInterruptibly(int arg) throws InterruptedException;public final boolean tryAcquireSharedNanos(int arg, long nanosTimeout) throws InterruptedException;public final boolean releaseShared(int arg); addWaiter：等待队列的加入605606607608609610611612613614615616617618619620621622private Node addWaiter(Node mode) { Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; if (pred != null) { /** * 通过CAS来更改队列tail结点。 * 注意：在并发访问时，这里的CAS成功，可以保证prev结点非null，但next结点有可能为null。 */ node.prev = pred; if (compareAndSetTail(pred, node)) { pred.next = node; return node; } } enq(node); return node;} 605606607608609610611612613614615616617618619620621622623624625626private Node enq(final Node node) { for (;;) { Node t = tail; if (t == null) { // Must initialize /** * 这里多了个初始化：也就是有需要时才初始化head结点。 */ if (compareAndSetHead(new Node())) tail = head; } else { /** * 通过CAS来更改队列tail结点。 * 注意：在并发访问时，这里的CAS成功，可以保证prev结点非null，但next结点有可能为null。 */ node.prev = t; if (compareAndSetTail(t, node)) { t.next = node; return t; } } }} 从上面的代码可以知道，结点的加入只是简单的通过CAS更新队列的tail字段：保证prev跟tail的原子更新，但不保证tail与next的原子更新。 acquire：互斥锁获取119611971198119912001201120212031204120512061207120812091210public final void acquire(int arg) { /* * 调用具体实现类的tryAcquire，如果返回true，则认为获取锁成功，当前函数返回； * 如果返回false，则将当前线程加入锁的等待队列（addWaiter，并且注意这里的加的 * 等待结点类型为Node.EXCLUSIVE，也就是互斥锁），当前线程会进入休眠（dormant） * 状态，并等待前继结点唤醒，然后重新竞争锁，直到获取锁后返回。 * * acquireQueued返回true说明线程在等待过程中被中断过（interrupted），则通过 * selfInterrupt（实际调用Thread.currentThread().interrupt()）重新 * interrupte当前线程以向调用者传递中断信号。 */ if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();} 855856857858859860861862863864865866867868869870871872873874875876877878879880881882883884885886887888889final boolean acquireQueued(final Node node, int arg) { boolean failed = true; try { boolean interrupted = false; for (;;) { final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) { /** * 只有在当前结点的前继结点为head时，当前结点去才会尝试获取锁。 * 获取锁成功时（tryAcquire返回true），将当前结点设置成head， * 并根据中断状态返回true或者false。 */ setHead(node); p.next = null; // help GC failed = false; return interrupted; } /** * shouldParkAfterFailedAcquire判断是否应该阻塞（park）当前线程，判断的依据是 * 前继结点的状态（p.waitStatus），只有该状态为Node.SIGNAL时才会阻塞当前线程： * 此状态说明，当前结点无法暂时获取锁，并且前继结点保证会在释放锁的时候唤醒当前线程。 * * parkAndCheckInterrupt的实现就比较简单了，调用LockSupport.park(this)阻塞 * 当前线程，并返回线程当前的中断状态。 */ if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; } } finally { if (failed) cancelAcquire(node); }} release：互斥锁的释放1259126012611262126312641265126612671268126912701271127212731274public final boolean release(int arg) { if (tryRelease(arg)) { Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) /** * 在head不为null，并且waitStatus不为0的情况下，唤醒后继结点：只是给后续结点一次 * 竞争锁的机会，后续结点未必能获取到锁。 * * unparkSuccessor的实现：找到h的后继结点，并调用LockSupport.unpark唤醒后继结点 * 对应的线程。 */ unparkSuccessor(h); return true; } return false;} acquireShared：共享锁获取946947948949950951952953public final void acquireShared(int arg) { /** * 调用具体实现类的tryAcquireShared，如果返回值不小于0，则认为获取共享锁成功； * 否则通过doAcquireShared调用进入等待锁逻辑。 */ if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg);} 946947948949950951952953954955956957958959960961962963964965966967968969970971972973974975976977978979980981982private void doAcquireShared(int arg) { final Node node = addWaiter(Node.SHARED); boolean failed = true; try { boolean interrupted = false; for (;;) { final Node p = node.predecessor(); if (p == head) { int r = tryAcquireShared(arg); if (r &gt;= 0) { /** * 仔细与上面的互斥锁的获取逻辑比较下，会发现逻辑基本差不多： * 前继结点为head，并且获取锁成功（与互斥锁不同的时tryAcquireShared返回值 * 不小于0时，认为获取锁成功）；不但要将当前结点设置为head结点，并且要将此事件 * 向后传递（setHeadAndPropagate）。 */ setHeadAndPropagate(node, r); p.next = null; // help GC if (interrupted) selfInterrupt(); failed = false; return; } } /** * 与互斥锁逻辑一致 */ if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; } } finally { if (failed) cancelAcquire(node); }} 708709710711712713714715716717718719720721722723724725726727728729730731private void setHeadAndPropagate(Node node, int propagate) { Node h = head; // Record old head for check below setHead(node); /* * Try to signal next queued node if: * Propagation was indicated by caller, * or was recorded (as h.waitStatus) by a previous operation * (note: this uses sign-check of waitStatus because * PROPAGATE status may transition to SIGNAL.) * and * The next node is waiting in shared mode, * or we don't know, because it appears null * * The conservatism in both of these checks may cause * unnecessary wake-ups, but only when there are multiple * racing acquires/releases, so most need signals now or soon * anyway. */ if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0) { Node s = node.next; if (s == null || s.isShared()) doReleaseShared(); }} setHeadAndPropagate除了将head设置为当前持有锁的结点外，还需要保证在后面这两种情况下向后传播可以获取锁的信息： propagate &gt; 0（也就是tryAcquireShared &gt; 0，表示后续的获取锁操作也可能成功）； 原始head结点的waitStatus &lt; 0，也就是以前有某个结点希望释放锁的操作向后传播。 releaseShared：共享锁的释放1339134013411342134313441345public final boolean releaseShared(int arg) { if (tryReleaseShared(arg)) { doReleaseShared(); return true; } return false;} 670671672673674675676677678679680681682683684685686687private void doReleaseShared() { for (;;) { Node h = head; if (h != null &amp;&amp; h != tail) { int ws = h.waitStatus; if (ws == Node.SIGNAL) { if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases unparkSuccessor(h); } else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS } if (h == head) // loop if head changed break; }} 可以看到，doReleaseShared需要保证两点： 要么至少唤醒一个等待的结点：waitStatus == Node.SIGNAL； 要么将当前head结点的waitStatus设置成Node.PROPAGATE，以保证在后续线程持有到锁后，可以向后传播此次释放锁事件（见setHeadAndPropagate的分析）。 具体锁实现ReentrantLock互斥模式，state代表互斥锁的状态：为0说明当前锁可用；为1说明当前锁已经被某个线程持有，其它线程必须等待。获取锁等价于将state设置成1；释放锁等价于将state设置为0。 公平锁获取236237238239240241242243244245246247248249250251252253254255256257258259260261262protected final boolean tryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) { /** * 只有在等待队列里没有前继等待线程时（!hasQueuedPredecessors）， * 当前线程才能尝试获取锁（更新锁状态：compareAndSetState(0, acquires)）， * 如果成功则将当前线程标记为锁持有者，并且返回true。 */ setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { /** * 处理重入逻辑：当前线程持有锁，并且又发起获取锁请求 */ int nextc = c + acquires; if (nextc &lt; 0) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; } return false;} 非公平锁获取672170672171672172protected final boolean tryAcquire(int acquires) { return nonfairTryAcquire(acquires);} 133134135136137138139140141142143144145146147148149150151152153154final boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { /** * 跟公平锁获取相比，这里没有判断是否有前继等待线程。也就是说当前线程可以在等待队列里 * 有线程在等待获取锁的时候，竞争成功并且持有锁，这对其它等待线程来说，就是不公平的。 */ if (compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; } return false;} ReentrantReadWriteLock共享互斥模式结合：写锁对应互斥锁，读锁对应共享锁。state被分为两部分：高16位代表读锁持有数量；低16位代表写锁持有数量。 主要的实现逻辑跟ReentrantLock类似，但因为同时有两个锁，所以有些不同： 在写锁被当前线程持有的情况下，其它线程不同持有任意锁； 在写锁被当前线程持有的情况下，当前线程可以继续请求获取读锁和写锁； 在读锁被当前线程持有的情况下，其它线程可以持有读锁，不能持有写锁； 在读锁被当前线程持有的情况下，当前线程和其它持有读锁的线程可以继续请求获取读锁，不能请求获取写锁。 代码就不详细说明了。 Semaphore共享模式，state代表许可的个数，初始为许可的个数，每一次的acquire，许可减1。注意：tryAcquireShared返回为int，这里会返回剩余的许可个数。 公平与非公平的处理与ReentrantLock处理逻辑类似，不再详细分析。 CountDownLatch共享模式，state代表count个数，初始为count个数。下面为核心代码： 177178179180181182183184185186187188189190191protected int tryAcquireShared(int acquires) { return (getState() == 0) ? 1 : -1;}protected boolean tryReleaseShared(int releases) { // Decrement count; signal when transition to zero for (;;) { int c = getState(); if (c == 0) return false; int nextc = c-1; if (compareAndSetState(c, nextc)) return nextc == 0; }} 可以看到，在初始情况下，所有的tryAcquireShared（CountDownLatch.await会调用此方法）都会阻塞（getState == count，不为0）；每一次的tryReleaseShared（CountDownLatch.countDown会调用此方法）将count减1，直到为0并且会返回true（nextc == 0），这时acquireShared会调用doReleaseShared唤醒被阻塞的线程（getState == 0保证tryAcquireShared肯定会成功）。 FutureTask共享模式，state代表任务的完成状态：0代表任务已经准备就绪，1代表任务正在运行，2代表任务已经完成，4代表任务取消。 223224225226227228229230231232233234235236237/** * Implements AQS base acquire to succeed if ran or cancelled */protected int tryAcquireShared(int ignore) { return innerIsDone() ? 1 : -1;}/** * Implements AQS base release to always signal after setting * final done status by nulling runner thread. */protected boolean tryReleaseShared(int ignore) { runner = null; return true;} 由上面代码可以看到在任务没有完成时，任何调用tryAcquireShared（FutureTask.get会调用此方法）的线程都会阻塞；tryReleaseShared永远返回true。 任务执行完成后，会将state设置成2（正常完成或者出现异常）或者4（任务被取消）：innerIsDone方法在这两种情况下都会返回true。","link":"/2013/08/31/java-concurrent-source-code-reading-1/"},{"title":"Java并发源码分析 - ThreadPoolExecutor","text":"为什么需要线程池？ 避免在运行大量任务时，频繁的线程创建和销毁开销； 使资源的使用得到有效控制，避免创建过多的线程占用系统资源。 基本概念Core and maximum pool sizes控制线程池核心线程数以及最大可生成的线程数量。是否需要创建线程与当前线程的数量以及任务队列的状态在关，后面会详述。 Keep-alive times 默认情况下，只有在当前worker线程数大于core大小的情况下，空闲一定时间的worker线程才可以被回收，但是也可以通过allowCoreThreadTimeOut(boolean)函数来控制core线程的超时时间。 任务队列ThreadPoolExecutor使用BlockingQueue来管理任务队列，任务队列与线程池大小的关系如下： 如果线程池数量小于corePoolSize，Executor倾向于新增worker线程； 如果线程池数量多于或者等于corePoolSize倾向于将任务放入队列； 如果任务队列已满，并且线程池数量还没有超过maximumPoolSize，那么新的worker线程； 如果任务队列已满，并且线程池数量已经超过maximumPoolSize，那么任务被reject； 实现提交任务1300130113021303130413051306130713081309131013111312131313141315131613171318131913201321132213231324132513261327132813291330public void execute(Runnable command) { if (command == null) throw new NullPointerException(); int c = ctl.get(); if (workerCountOf(c) &lt; corePoolSize) { /** * 如果当前worker数量小于corePoolSize，则创建新的worker。 */ if (addWorker(command, true)) return; c = ctl.get(); } /** * 尝试将任务添加到任务队列。 */ if (isRunning(c) &amp;&amp; workQueue.offer(command)) { int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); } /** * 在worker数量大于corePoolSize，并且任务添加到队列失败（队列满）的情况下，尝试创建新的worker， * 如果创建失败表示已经达到maximumPoolSize，则reject任务。 */ else if (!addWorker(command, false)) reject(command);} 创建worker线程去除一些状态检查后，核心代码如下： 886887888889890891892893894895896897898899900901902903904905private boolean addWorker(Runnable firstTask, boolean core) { Worker w = new Worker(firstTask); Thread t = w.thread; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { workers.add(w); int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s; } finally { mainLock.unlock(); } t.start(); return true;} 可以看到，很简单，创建一个Worker线程，将他加到workers集合中，然后启动对应worker线程，DONE。 我们来看看Worker的定义： 575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633private final class Worker extends AbstractQueuedSynchronizer implements Runnable{ /** * This class will never be serialized, but we provide a * serialVersionUID to suppress a javac warning. */ private static final long serialVersionUID = 6138294804551838833L; /** Thread this worker is running in. Null if factory fails. */ final Thread thread; /** Initial task to run. Possibly null. */ Runnable firstTask; /** Per-thread task counter */ volatile long completedTasks; /** * Creates with given first task and thread from ThreadFactory. * @param firstTask the first task (null if none) */ Worker(Runnable firstTask) { this.firstTask = firstTask; this.thread = getThreadFactory().newThread(this); } /** Delegates main run loop to outer runWorker */ public void run() { runWorker(this); } // Lock methods // // The value 0 represents the unlocked state. // The value 1 represents the locked state. protected boolean isHeldExclusively() { return getState() == 1; } protected boolean tryAcquire(int unused) { if (compareAndSetState(0, 1)) { setExclusiveOwnerThread(Thread.currentThread()); return true; } return false; } protected boolean tryRelease(int unused) { setExclusiveOwnerThread(null); setState(0); return true; } public void lock() { acquire(1); } public boolean tryLock() { return tryAcquire(1); } public void unlock() { release(1); } public boolean isLocked() { return isHeldExclusively(); }} 除去跟锁定义相关的代码后，核心就是run函数的实现：调用runWorker运行Worker线程的运行逻辑。 Worker线程运行逻辑109810991100110111021103110411051106110711081109111011111112111311141115111611171118111911201121112211231124112511261127112811291130final void runWorker(Worker w) { Runnable task = w.firstTask; w.firstTask = null; boolean completedAbruptly = true; try { while (task != null || (task = getTask()) != null) { w.lock(); clearInterruptsForTaskRun(); try { beforeExecute(w.thread, task); Throwable thrown = null; try { task.run(); } catch (RuntimeException x) { thrown = x; throw x; } catch (Error x) { thrown = x; throw x; } catch (Throwable x) { thrown = x; throw new Error(x); } finally { afterExecute(task, thrown); } } finally { task = null; w.completedTasks++; w.unlock(); } } completedAbruptly = false; } finally { processWorkerExit(w, completedAbruptly); }} 就是一个while循环，在有任务的情况下（两种：一种在创建Worker线程时传入，由firtstTask传入；一种通过getTask由任务队列获取），执行任务，并调用设置的回调函数（beforeExecute，afterExecute等）。 我们来看看getTask的实现： 109810991100110111021103110411051106110711081109111011111112private Runnable getTask() { boolean timedOut = false; // Did the last poll() time out? for (;;) { try { Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; timedOut = true; } catch (InterruptedException retry) { timedOut = false; } }} 去除了状态检查的相关代码后，核心的逻辑如下：在需要处理超时的情况下调用BlockingQueue.poll来获取任务，如果在超时后还没有任务，则让相应的worker线程退出；如果不需要处理超时时候，调用BlockingQueue.take，阻塞当前worker线程一直到有任务到达。 总结ThreadPoolExecutor会根据线程池状态和任务队列状态创建worker线程，而每个worker线程的主要任务就是不断的去任务队列里去拿任务：要么一直阻塞等，要么超时后退出；拿到任务后，运行任务并调用相关回调。","link":"/2013/09/13/java-concurrent-source-code-reading-2/"},{"title":"JVM 中可生成的最大 Thread 数量","text":"背景最近想测试下Openfire下的最大并发数，需要开大量线程来模拟客户端。对于一个JVM实例到底能开多少个线程一直心存疑惑，所以打算实际测试下，简单google了把，找到影响线程数量的因素有下面几个： -Xms intial java heap size -Xmx maximum java heap size -Xss the stack size for each thread 系统限制 系统最大可开线程数 测试程序1234567891011121314151617181920212223import java.util.concurrent.atomic.AtomicInteger;public class TestThread extends Thread { private static final AtomicInteger count = new AtomicInteger(); public static void main(String[] args) { while (true) { (new TestThread()).start(); } } @Override public void run() { System.out.println(count.incrementAndGet()); while (true) try { Thread.sleep(Integer.MAX_VALUE); } catch (InterruptedException e) { break; } }} 测试环境系统：Ubuntu 10.04 Linux Kernel 2.6 （32位）内存：2GJDK：1.7 测试结果不考虑系统限制 -Xms -Xmx -Xss 结果 1024m 1024m 1024k 1737 1024m 1024m 64k 26077 512m 512m 64k 31842 256m 256m 64k 31842 在创建的线程数量达到31842个时，系统中无法创建任何线程。 由上面的测试结果可以看出增大堆内存（-Xms，-Xmx）会减少可创建的线程数量，增大线程栈内存（-Xss，32位系统中此参数值最小为60K）也会减少可创建的线程数量。 结合系统限制线程数量31842的限制是是由系统可以生成的最大线程数量决定的：/proc/sys/kernel/threads-max，可其默认值是32080。修改其值为10000：echo 10000 &gt; /proc/sys/kernel/threads-max，修改后的测试结果如下： -Xms -Xmx -Xss 结果 256m 256m 64k 9761 这样的话，是不是意味着可以配置尽量多的线程？再做修改：echo 1000000 &gt; /proc/sys/kernel/threads-max，修改后的测试结果如下： -Xms -Xmx -Xss 结果 256m 256m 64k 32279 128m 128m 64k 32279 发现线程数量在达到32279以后，不再增长。查了一下，32位Linux系统可创建的最大pid数是32678，这个数值可以通过/proc/sys/kernel/pid_max来做修改（修改方法同threads-max），但是在32系统下这个值只能改小，无法更大。在threads-max一定的情况下，修改pid_max对应的测试结果如下： pid_max -Xms -Xmx -Xss 结果 1000 128m 128m 64k 582 10000 128m 128m 64k 9507 在Windows上的情况应该类似，不过相比Linux，Windows上可创建的线程数量可能更少。基于线程模型的服务器总要受限于这个线程数量的限制。 总结JVM中可以生成的最大数量由JVM的堆内存大小、Thread的Stack内存大小、系统最大可创建的线程数量三个方面影响。具体数量可以根据Java进程可以访问的最大内存（32位系统上一般2G）、堆内存、Thread的Stack内存来估算。 续在64位Linux系统（CentOS 6， 3G内存）下测试，发现还有一个参数是会限制线程数量：max user process（可通过ulimit –a查看，默认值1024，通过ulimit –u可以修改此值），这个值在上面的32位Ubuntu测试环境下并无限制。将threads-max，pid_max，max user process，这三个参数值都修改成100000，-Xms，-Xmx尽量小（128m，64m），-Xss尽量小（64位下最小104k，可取值128k）。事先预测在这样的测试环境下，线程数量就只会受限于测试环境的内存大小（3G），可是实际的测试结果是线程数量在达到32K（32768，创建的数量最多的时候大概是33000左右）左右时JVM是抛出警告：Attempt to allocate stack guard pages failed，然后出现OutOfMemoryError无法创建本地线程。查看内存后发现还有很多空闲，所以应该不是内存容量的原因。Google此警告无果，暂时不知什么原因，有待进一步研究。 续2今天无意中发现文章[7]，马上试了下，果然这个因素会影响线程创建数量，按文中描述把/proc/sys/vm/max_map_count的数量翻倍，从65536变为131072，创建的线程总数量达到65000+，电脑基本要卡死（3G内存）… 简单查了下这个参数的作用，在[8]中的描述如下： This file contains the maximum number of memory map areas a process may have. Memory map areas are used as a side-effect of calling malloc, directly by mmap and mprotect, and also when loading shared libraries. While most applications need less than a thousand maps, certain programs, particularly malloc debuggers, may consume lots of them, e.g., up to one or two maps per allocation. The default value is 65536. OK，这个问题总算完满解决，最后总结下影响Java线程数量的因素： Java虚拟机本身：-Xms，-Xmx，-Xss；系统限制：/proc/sys/kernel/pid_max，/proc/sys/kernel/thread-max，max_user_process（ulimit -u），/proc/sys/vm/max_map_count。 参考资料 http://blog.krecan.net/2010/04/07/how-many-threads-a-jvm-can-handle/ http://www.cyberciti.biz/tips/maximum-number-of-processes-linux-26-kernel-can-handle.html http://geekomatic.ch/2010/11/24/1290630420000.html http://stackoverflow.com/questions/763579/how-many-threads-can-a-java-vm-support http://www.iteye.com/topic/1035818 http://hi.baidu.com/hexiong/blog/item/16dc9e518fb10c2542a75b3c.html https://listman.redhat.com/archives/phil-list/2003-August/msg00025.html http://www.linuxinsight.com/proc_sys_vm_max_map_count.html","link":"/2011/08/09/jvm-max-threads/"},{"title":"RabbitMQ HA机制","text":"RabbitMQ为了保证消息不丢失，提供了高可用机制，或者称为镜像队列，详细文档可以参考这里，本文试图搞清楚其实现细节。 创建高可用队列RabbitMQ在3.x之前是通过客户端在创建队列时传入特定参数还创建高可用队列的，3.x之后，所有高可用队列都是通过policy来管理，使用类似正则匹配的方式来决定哪些队列需要创建成镜像队列。 与普通队列的差别普通队列只在创建结点上存在一个Erlang进程（amqqueue_process）来处理消息逻辑，而HA的队列存在两类进程：master进程（amqqueue_process）和slave进程（rabbit_mirror_queue_slave），每个进程包含一个实际用于处理消息逻辑的队列（rabbit_variable_queue）。整体结构如下图： 消息流程发送消息生产消息的事件会通过rabbit_channel进程同时广播到master和slave进程（异步），并且在master进程收到消息后，会再通过GM将该消息广播到所有slave进程（异步），也就是说对于生产消息的事件，slave进程会同时收到两个消息：一个从GM发来，一个从rabbit_channel进程发来。消息流如下图所示： 代码里的文档对于为什么同时需要从channel发送消息到slave的解释如下： The key purpose of also sending messages directly from the channelsto the slaves is that without this, in the event of the death ofthe master, messages could be lost until a suitable slave ispromoted. However, that is not the only reason. A slave cannot sendconfirms for a message until it has seen it from thechannel. Otherwise, it might send a confirm to a channel for amessage that it might never receive from that channel. This canhappen because new slaves join the gm ring (and thus receivemessages from the master) before inserting themselves in thequeue’s mnesia record (which is what channels look at for routing).As it turns out, channels will simply ignore such bogus confirms,but relying on that would introduce a dangerously tight coupling. 也就是说不通过channel发送消息到slave进程可能会产生两个问题： 如果master进程挂掉了，消息有可能会丢失：master收到消息，广播到slave进程之前挂掉，slave进程就不可能通过GM收到该消息； 在slave进程已经加入到GM中，但是slave进程信息还没有写到mnesia数据库中时，slave进程可能只会收到从GM发送过来的消息，这时候，slave会发送一个从来没收到过的消息的confirm消息到channel进程；从上面的解释来看，RabbitMQ认为这样会带来强耦合的关系。 confirm消息master进程及slave进程在实际队列完成消息入队工作（可能会持久化到磁盘）后，将会发送进程（rabbit_channel）发送一个confirm消息，rabbit_channel进程只有在收到所有队列进程（master及slave）的confirm消息后，才会向客户端发回confirm消息。 消费消息所有消费消息的相关事件（获取消息，ack消息，requeue消息）都是只发送到master进程，然后由master进程通过GM来广播这些事件到所有slave进程。消息流如下图所示： 节点变化新结点加入新的slave结点可以随时加入集群，但是加入之前的消息并不会同步到新的slave结点。也就是说在一开始，新的slave结点肯定会在一段时间内与master的内容不同步，随着旧消息被消费，新slave结点的内容会保持与master同步。 Slave挂掉基本无影响，连接在这个slave上的客户端需要重新连接到其它结点。 Master挂掉 一个slave会被选举为新的master，要求这个slave为所有slave中最老的结点； Slave认为所有之前的Consumer都突然断开，然后会requeue所有之前未ACK的消息（ACK可能未到已挂掉的Master或者已经到已挂掉的Master，但在广播到到Slave之前，Master挂掉），这种情况下，会导致客户端收到重复的消息； 未断开的Consumer会收到 Consumer Cancellation Notification，这时候Consumer应该重新订阅队列。 也就是说master结点的异常会产生两个问题：1）可能会丢消息；2）可能会收到重复消息。重复消息还可以接受（就算是普通队列也会面临这个问题，需要应用层来处理），但是丢消息对应用来说可能就会有点问题。 运维网络分区RabbitMQ提供了一个配置参数：cluster_partition_handling，可选值有三个：ignore，pause_minority，autoheal，具体什么意思可以参考这里。 也可以自己手动来解决：在发生网络分区时，选一个分区，把另一个分区的RabbitMQ全部重启一遍就可以重新组成集群。按官方的意思，在这之后，最好把整个集群重启一次才能清除掉警告信息。","link":"/2013/11/12/rabbitmq-ha/"},{"title":"RabbitMQ持久化机制","text":"之前其实已经写过一篇关于RabbitMQ持久化的文章，但那篇文章侧重代码层面的写入流程，对于持久化操作何时发生以及什么时候会刷新到磁盘等问题其实都没有搞清楚，这篇文章着重于关注这些问题。 消息什么时候需要持久化？根据官方博文的介绍，RabbitMQ在两种情况下会将消息写入磁盘： 消息本身在publish的时候就要求消息写入磁盘； 内存紧张，需要将部分内存中的消息转移到磁盘； 消息什么时候会刷到磁盘？ 写入文件前会有一个Buffer，大小为1M（1048576），数据在写入文件时，首先会写入到这个Buffer，如果Buffer已满，则会将Buffer写入到文件（未必刷到磁盘）； 有个固定的刷盘时间：25ms，也就是不管Buffer满不满，每隔25ms，Buffer里的数据及未刷新到磁盘的文件内容必定会刷到磁盘； 每次消息写入后，如果没有后续写入请求，则会直接将已写入的消息刷到磁盘：使用Erlang的receive x after 0来实现，只要进程的信箱里没有消息，则产生一个timeout消息，而timeout会触发刷盘操作。 消息在磁盘文件中的格式消息保存于$MNESIA/msg_store_persistent/x.rdq文件中，其中x为数字编号，从1开始，每个文件最大为16M（16777216），超过这个大小会生成新的文件，文件编号加1。消息以以下格式存在于文件中： &lt;&lt;Size:64, MsgId:16/binary, MsgBody&gt;&gt; MsgId为RabbitMQ通过rabbit_guid:gen()每一个消息生成的GUID，MsgBody会包含消息对应的exchange，routing_keys，消息的内容，消息对应的协议版本，消息内容格式（二进制还是其它）等等。 文件何时删除？当所有文件中的垃圾消息（已经被删除的消息）比例大于阈值（GARBAGE_FRACTION = 0.5）时，会触发文件合并操作（至少有三个文件存在的情况下），以提高磁盘利用率。 publish消息时写入内容，ack消息时删除内容（更新该文件的有用数据大小），当一个文件的有用数据等于0时，删除该文件。 消息索引什么时候需要持久化？索引的持久化与消息的持久化类似，也是在两种情况下需要写入到磁盘中：要么本身需要持久化，要么因为内存紧张，需要释放部分内存。 消息索引什么时候会刷到磁盘？ 有个固定的刷盘时间：25ms，索引文件内容必定会刷到磁盘； 每次消息（及索引）写入后，如果没有后续写入请求，则会直接将已写入的索引刷到磁盘，实现上与消息的timeout刷盘一致。 消息索引在磁盘文件中的格式每个队列会对队列中的消息维护一个索引，每入队列一个消息，索引加1，索引在持久化时，以2^14个（16384）entry为单位组成一个文件（Segment）。索引在写入时，未必会按序写入，为了避免过多的磁盘寻址，索引信息会首先保存在Journal文件中，当该文件中的entry数量达到65536个时，会将其中的内容写入到Segment文件中。其中Journal保存于$MNESIA/queues/md5(queueName)/journal.jif文件中，索引保存于$MNESIA/queues/md5(queueName)/x.idx文件中，其中的x为数字编号，类似于消息文件的编号。 索引信息在Segment文件中的格式有两种： 对应publish消息：&lt;&lt;1:1, IsPersistent:1, RelSeq:14, MsgId:16/binary, Expiry:8/binary&gt;&gt;； 对应deliver或者ack消息：&lt;&lt;01:2, RelSeq:14&gt;&gt;。 文件何时删除？rabbit_msg_index模块为每一个Segment维护一个unacked计数，每publish一个消息加1，每ack一个消息减1，当unacked=0时，文件删除。 注：持久化文件的大小可通过配置参数msg_store_file_size_limit修改，journal文件中最大entry数量可通过参数queue_index_max_ journal_entries配置，具体参见这里","link":"/2013/11/15/rabbitmq-persistent/"},{"title":"小模型（Small Language Model）信息汇总","text":"随着人工智能领域的不断发展，语言模型也变得越来越重要。在这个领域中，小模型（Small Language Models）正逐渐崭露头角，与大模型（Large Language Models）相比，它们有着独特的特点和潜在的价值。这篇文章主要关注两点，模型的进展，推理的进展，特别是在消费级设备（PC，移动设备）相关的进展。 概念小模型与大模型的区别 规模大小：最明显的差异是规模大小。大模型通常包括数百亿甚至上千亿个参数，而小模型则包含数十亿甚至更少的参数。 计算资源：大模型需要庞大的计算资源来进行训练和推理，这使得它们在运行时需要强大的硬件支持。相比之下，小模型可以在更加普通的硬件上运行，这使得它们更具可行性。 小模型的优势和价值 计算成本：小模型在计算成本上具有优势，因为它们不需要大规模的硬件支持。这使得它们成为中小企业、初创公司和研究团队的理想选择，因为它们可以更轻松地实现自然语言处理的功能。 更容易控制和定制：小模型更容易进行定制和控制，因为它们的规模较小，更容易理解和调整。这使得它们适用于需要特定定制解决方案的应用领域。 可能的问题虽然小模型具有很多优势，但也存在一些潜在的问题： 性能限制：由于其较小的规模，小模型在某些任务上可能表现不如大模型。它们的语言理解和生成能力可能会受到限制，特别是在处理复杂或专业性的任务时。 知识覆盖范围：小模型的训练数据较小，因此它们的知识覆盖范围可能有限。对于某些领域的信息，它们可能无法提供足够的帮助。 模型进展Phi-2微软在 23.12 发布了 Phi-2： Phi-2, a small language model (SLM) with 2.7 billion parameters.Phi-2 is notable for its exceptional performance on various benchmarks, rivaling or surpassing models up to 25 times larger in size.This achievement is attributed to innovative training approaches, including the use of high-quality “textbook-quality” data and techniques for scaling knowledge transfer.Despite its smaller size, Phi-2 demonstrates advanced reasoning and language understanding capabilities. 一个只有 2.7B 参数的模型，但效果与 llama 2 70B 及 Mistral 7B 差不多： 训练数据规模 Phi-2 is a Transformer-based model with a next-word prediction objective, trained on 1.4T tokens from multiple passes on a mixture of Synthetic and Web datasets for NLP and coding.The training for Phi-2 took 14 days on 96 A100 GPUs. 做个对比： GPT-3: 174B parameters. Phi-2: 2.7B parameters.GPT-3: trained with 300B tokens. Phi-2: trained with 1400B tokens. Key Insights Behind Phi-2 Firstly, training data quality plays a critical role in model performance. This has been known for decades, but we take this insight to its extreme by focusing on “textbook-quality” data, following upon our prior work “Textbooks Are All You Need.”Secondly, we use innovative techniques to scale up, starting from our 1.3 billion parameter model, Phi-1.5, and embedding its knowledge within the 2.7 billion parameter Phi-2. 看起来除了技术上的革新外，最重要的就是用高质量的数据训练模型。 Phi-2 一开始出来时候，licensed 只能用于研究，但是在前几天改成了 MIT，就是说随便用。国内已经有学校基于 Phi-2 搞多模态模型了。 Mistral 7BMistral 7B 现在在开源社区用的比较多，很多人基于 Mistral 7B 做一些小的应用，比如 private local 的个人助理之类的。 Mistral 7B 是迄今为止同等规模中最强大的语言模型。作为一个拥有7.3亿参数的模型，Mistral 7B在所有基准测试中均优于13亿参数的Llama 2，并在许多基准测试中超越了34亿参数的Llama 1。更令人瞩目的是，Mistral 7B在代码领域的表现接近7B的CodeLlama，同时在英语任务上依然表现出色。Mistral 7B采用了两项创新技术：分组查询注意力（Grouped-query Attention, GQA）和滑动窗口注意力（Sliding Window Attention, SWA），使得其在处理更长序列时成本更低，推理速度更快。此外，它还在Apache 2.0许可下发布，无限制地供用户使用。这款模型不仅易于在任何任务上进行微调，而且在聊天方面的微调模型甚至超越了Llama 2的13亿参数聊天模型。Mistral 7B的表现在多个领域均十分出色，尤其在常识推理、世界知识、阅读理解、数学和编码等多个基准测试中显示了显著的优势。特别值得一提的是，在推理、理解和STEM（科学、技术、工程和数学）推理（MMLU）方面，Mistral 7B的表现相当于其三倍大小的Llama 2模型，这意味着在内存和吞吐量方面的显著节省。Mistral 7B的滑动窗口注意力（SWA）机制是其主要的创新点，每层关注前4,096个隐藏状态，实现了线性计算成本。该模型在序列长度为16k和窗口大小为4k的情况下，实现了2倍的速度提升。此外，固定的注意力范围意味着可以将缓存限制在sliding_window大小的标记中，这在不影响模型质量的情况下，节省了一半的缓存内存。 Mistral 还有个 8*7B 的 MoE（Mixtral of Experts）模型，效果也是很不错，号称现在最强开源模型。 什么是 MoE？ Sparse MoE layers are used instead of dense feed-forward network (FFN) layers. MoE layers have a certain number of “experts” (e.g. 8), where each expert is a neural network. In practice, the experts are FFNs, but they can also be more complex networks or even a MoE itself, leading to hierarchical MoEs!A gate network or router, that determines which tokens are sent to which expert. 简单讲就是一个门控网络+多个“专家“网络，门控网络负责将 token 路由到一个或者多个”专家“网络。 推理进展有了小的合适的模型，那怎么在消费级设备上运行大模型？ Machine Learning Compilation for LLMMLC 是 陈天奇 出的一个针对 LLM 做的一个推理部署解决方案，大致的逻辑如下图，包含三个部分： Model definition in Python. MLC offers a variety of pre-defined architectures, such as Llama (e.g., Llama2, Vicuna, OpenLlama, Wizard), GPT-NeoX (e.g., RedPajama, Dolly), RNNs (e.g., RWKV), and GPT-J (e.g., MOSS). Model developers could solely define the model in pure Python, without having to touch code generation and runtime. Model compilation in Python. Models are compiled by TVM Unity compiler, where the compilation is configured in pure Python. MLC LLM quantizes and exports the Python-based model to a model library and quantized model weights. Quantization and optimization algorithms can be developed in pure Python to compress and accelerate LLMs for specific usecases. Platform-native runtimes. Variants of MLCChat are provided on each platform: C++ for command line, Javascript for web, Swift for iOS, and Java for Android, configurable with a JSON chat config. App developers only need to familiarize with the platform-naive runtimes to integrate MLC-compiled LLMs into their projects. 关键的三个部分：1）标准化的模型定义；2）模型编译及优化；3）平台相关的运行时（比如针对 iOS，Android等的运行时）。看起来像不像 Java 的逻辑：字节码，编译器，平台相关的运行时。所以可以叫跨平台大模型推理？ llama.cppWhy llama.cpp? However, practical applications of LLMs can be limited by the need for high-powered computing or the necessity for quick response times. These models typically require sophisticated hardware and extensive dependencies, which can make difficult their adoption in more constrained environments.This is where LLaMa.cpp (or LLaMa C++) comes to the rescue, providing a lighter, more portable alternative to the heavyweight frameworks. 虽然 llama.cpp 一开始的目标是在 MacBook 上运行开源模型 LLaMA，但实际上发展到现在不仅限于此，也支持 Windows，Linux 等 PC 平台，以及 iOS，Android 等移动平台，支持的模型也越来越多。 AppleCoreML Apple CoreML是苹果公司专为iOS和macOS设备开发的一款强大的机器学习框架。这个框架的核心目标是使开发者能够轻松地在苹果设备上集成和运行各种机器学习模型，从而提供丰富和智能的用户体验。CoreML的设计注重高效性和易用性。它支持多种流行的机器学习模型，如神经网络、决策树、支持向量机等，同时还提供了一系列的工具和接口，方便开发者将训练好的模型转换为CoreML格式并集成到应用中。这意味着开发者可以在其他平台（例如TensorFlow或PyTorch）上训练模型，然后轻松地迁移到iOS设备上。CoreML的另一个亮点是其对硬件的优化。它能够充分利用苹果设备上的CPU、GPU以及专门的神经网络硬件加速器（如Apple Neural Engine），从而实现高效的模型运行。这种优化确保了应用在执行机器学习任务时的响应速度和能效。除了性能优化，CoreML还特别重视隐私保护。所有的模型推理操作都在设备本地完成，不需要将用户数据发送到云端。这不仅加快了处理速度，也为用户数据的安全和隐私提供了额外的保障。 LLM in a FlashApple 曾在 23.12 月份发表了一篇论文：LLM in a Flash，尝试用更小的内存运行尽可能大的模型。 The paper “LLM in a Flash: Efficient Large Language Model Inference with Limited Memory” explores methods to efficiently run large language models (LLMs) on devices with limited memory.The key focus is on using flash memory for storing model parameters, with selective loading into DRAM based on demand.Two main techniques are introduced: “windowing,” which reduces data transfer by reusing previously activated neurons, and “row-column bundling,” which optimizes data chunk sizes for flash memory reading.These methods enable running models up to twice the size of available DRAM with significantly increased inference speed.This approach paves the way for effective LLM inference on devices with limited memory, enhancing model accessibility and application potential in resource-constrained environments. Stable Diffusion on Apple Silicon早在 22.12 月 Apple 就发表了一篇文章关于如何在 macOS 或者 iOS 上运行 Stable Diffusion：Stable Diffusion with Core ML on Apple Silicon There are a number of reasons why on-device deployment of Stable Diffusion in an app is preferable to a server-based approach.First, the privacy of the end user is protected because any data the user provided as input to the model stays on the user’s device.Second, after initial download, users don’t require an internet connection to use the model.Finally, locally deploying this model enables developers to reduce or eliminate their server-related costs. 为啥要在个人设备上运行 SD：1）隐私；2）不需要网络；3）成本。苹果提供了一个 Python 包可以帮你把 PyTorch 模型转换成 CoreML 模型，以及一个可以用于部署模型的 Swift 包。 个人想法 苹果在大模型上没啥声音，是不是在憋大招？现在的大模型准入门槛太高，大部分的团队及个人都玩不起，但如果苹果的设备上带了一个性能不错的小模型，会带来啥变化？如果小模型被手机或者设备厂商以标准化的服务或者接口提供出来，应该会催生很多 AI Native 的应用； 小模型在隐私及成本上有先天的优势，模型所需要的计算资源由用户提供。现在各个大厂出的应用（通义千问，文心一言，ChatGPT等）现在都是免费随便用，但这背后都需要大量资源或者钱来支撑，用户有多大的意愿持续付钱？AI 时代的工具类应用未必会基于大模型，而有可能基于小模型，大模型所需要的算力成本在目前这个经济下行的超势下，可能无法支撑工具类应用大规模传播。另一方面，类似个人助理这样的应用，有一些场景有很强的隐私性，小模型不需要网络的特性很可能更合适； 小模型的知识覆盖问题能不能用RAG解决？或者有更好更强的解决方案？ 以后也许会出现两条战线：一条向上卷越来越多的参数，更广的知识覆盖，更高的精度，更好的效果，可能适合 2B 及一些对效果有强要求的场景；另一条卷更高质量的训练数据及更强的推理能力，用更小的模型结合RAG之类的技术提供不错的效果，可能适合2C； 很有可能以后占据手机的不是大模型，而是小模型，手机厂商可能会成为赢家（华为真是赢麻了。。。）。","link":"/2024/01/14/small-language-model/"},{"title":"Java并发源码分析 - ForkJoin框架","text":"功能根据Java文档描述，ForkJoinPool中一种特殊的ExecutorService，可以执行ForkJoinTask。ForJoinTask可以在运行时Fork子任务，并join子任务的完成，本质上类似分治算法：将问题尽可能的分割，直到问题可以快速解决。对ForkJoinPool来说，与其它ExecutorService最重要的不同点是，它的工作线程会从其它工作线程的任务队列偷任务来执行。 实现根据代码里的文档，可以了解到ForkJoin框架主要由三个类组成： ForkJoinPool：管理worker线程，类似ThreadPoolExecutor，提供接口用于提交或者执行任务； ForkJoinWorkerThread：worker线程，任务保存在一个deque中； ForkJoinTask：ForkJoin框架中运行的任务，可以fork子任务，可以join子任务完成。 任务队列的管理ForkJoinPool及ForkJoinWorkerThread都有维护一个任务队列，ForkJoinPool用这个队列来保存非worker线程提交的任务，而ForkJoinWorkerThread则保存提交到本worker线程的任务。 任务队列以deque的形式存在，不过只通过三种方式访问其中的元素：push，pop，deq，其中push和pop只会由持有该队列的线程访问，而deq操作则是否由其它worker线程来访问。对应到代码上则是： ForkJoinTask&lt;?&gt;[] queue：代表任务队列，环形数组； int queueTop：队列头，push或者pop操作时，修改此值，因为只会被当前worker线程访问，所以是普通变量； volatile int queueBase：队列尾部，deq操作时修改此值，会有多个线程访问，使用volatile。 数据元素访问123long u = (((s = queueTop) &amp; (m = q.length - 1)) &lt;&lt; ASHIFT) + ABASE;UNSAFE.putOrderedObject(q, u, t);queueTop = s + 1; // or use putOrderedInt 上面的代码是从入队操作中的一段，前文提到queueTop保存队列头，那为什么不直接用queue[queueTop]=t来赋值就行了？了解原因之前，先来看看这两行代码在做什么： 12(s = queueTop) &amp; (m = q.length - 1) // queueTop % (q.length - 1)，也就是queueTop根据队列长度取模， // 取模后，就是队列头实际在数组中的索引； 那 Index &lt;&lt; ASHIFT + ABASE在算什么？先看看ASHIFT及ABASE的定义： 983984985986987988989990991992993994995996static { int s; try { UNSAFE = sun.misc.Unsafe.getUnsafe(); Class a = ForkJoinTask[].class; ABASE = UNSAFE.arrayBaseOffset(a); s = UNSAFE.arrayIndexScale(a); } catch (Exception e) { throw new Error(e); } if ((s &amp; (s-1)) != 0) throw new Error(&quot;data type scale not a power of two&quot;); ASHIFT = 31 - Integer.numberOfLeadingZeros(s);} 再来看看UNSAFE.arrayBaseOffset及UNSAFE.arrayIndexScale的文档： public native int arrayBaseOffset(Class arrayClass) Report the offset of the first element in the storage allocation of agiven array class. If #arrayIndexScale returns a non-zero valuefor the same class, you may use that scale factor, together with thisbase offset, to form new offsets to access elements of arrays of thegiven class. public native int arrayIndexScale(Class arrayClass) Report the scale factor for addressing elements in the storageallocation of a given array class. However, arrays of “narrow” typeswill generally not work properly with accessors like #getByte(Object, int) , so the scale &gt; &gt; factor for such classes is reported as zero. Java数组在实际存储时有一个对象头，后面才是实际的数组数据，而UNSAFE.arrayBaseOffset就是用来获取实际数组数据的偏移量；UNSAFE.arrayIndexScale则是获取对应数组元素占的字节数。这里的代码ABASE=16（数组对象头大小），s=4（ForkJoinTask对象引用占用字节数），ASIFT=2。 所以上面的Index &lt;&lt; ASHIFT + ABASE合起来就是Index左移2位=Index*4，也就是算Index的在数组中的偏移量，再加上ABASE就是Index在对象中的偏移量。也就是那一行代码主要就是算出来queueTop在队列数组中的实际偏移量，知道了这些，我们再来看第二行代码： 1UNSAFE.putOrderedObject(q, u, t); UNSAFE.putOrderedObject的文档： public native void putOrderedObject(Object o,long offset, Object x) Version of #putObjectVolatile(Object, long, Object)that does not guarantee immediate visibility of the store toother threads. This method is generally only useful if theunderlying field is a Java volatile (or if an array cell, onehat is otherwise only accessed using volatile accesses). 看的不明不白，找了下资料，这篇文章及这里解释的比较清楚： Unsafe.putOrderedObject guarante that writes will not be re-orderd by instructionreordering. Under the covers it uses the faster store-store barrier, rather than the theslower store-load barrier, which is used when doing a volatile write. write may be reordered with subsequent operations (or equivalently, might not be visible toother threads) until some other volatile write or synchronizing action occurs) 也就是说能够保证写写不会被重排序，但是不保证写会对其它线程可见，而volatile既保证写写不会被重排序，也保证写后对其它线程立即可见。可见Unsafe.putOrderedObject会比直接的volatile变量赋值速度会一点，这篇文章则指出Unsafe.putOrderedObject会比volatile写快3倍。 了解清楚这两行代码的作用后，再来回答一开始提出的问题，为什么要这么用？结合代码中的文档及自己的理解，我觉得原因无非两点： 需要保证写入元素的顺序对其它worker线程一致，也就是不会产生写写重排序； 不需要保证写读是否重排序，因为如果其它worker线程需要从当前队列steal任务，那么首先必须得个性volatile字段queueBase，而volatile的语义保证读之前的所有写操作的可见性，而Unsafe.putOrderedObject性能明显要好于volatile写。 不知道上面的理解是否正确，如有问题，请指正。 好吧，两行代码包含这么多的知识点。 容量初始容量 1&lt;&lt;13，最大容量 1&lt;&lt;24，队列满时，以2倍的方式增长，所以容量一直是2的幂次方。下面是扩容时的代码： 477478479480481482483484485486487488489490491492493494495496497498499500501502/** * Creates or doubles queue array. Transfers elements by * emulating steals (deqs) from old array and placing, oldest * first, into new array. */private void growQueue() { ForkJoinTask&lt;?&gt;[] oldQ = queue; int size = oldQ != null ? oldQ.length &lt;&lt; 1 : INITIAL_QUEUE_CAPACITY; if (size &gt; MAXIMUM_QUEUE_CAPACITY) throw new RejectedExecutionException(&quot;Queue capacity exceeded&quot;); if (size &lt; INITIAL_QUEUE_CAPACITY) size = INITIAL_QUEUE_CAPACITY; ForkJoinTask&lt;?&gt;[] q = queue = new ForkJoinTask&lt;?&gt;[size]; int mask = size - 1; int top = queueTop; int oldMask; if (oldQ != null &amp;&amp; (oldMask = oldQ.length - 1) &gt;= 0) { for (int b = queueBase; b != top; ++b) { long u = ((b &amp; oldMask) &lt;&lt; ASHIFT) + ABASE; Object x = UNSAFE.getObjectVolatile(oldQ, u); if (x != null &amp;&amp; UNSAFE.compareAndSwapObject(oldQ, u, x, null)) UNSAFE.putObjectVolatile (q, ((b &amp; mask) &lt;&lt; ASHIFT) + ABASE, x); } }} 有了开始的分析，这段代码就比较容易理解了： 从queueBase开始直到queueTop，通过UNSAFE.getObjectVolatile读取对应位置的元素； 通过UNSAFE.compareAndSwapObject将对应位置的元素设置为null； 如果上述CAS成功，则通过UNSAFE.putObjectVolatile将该元素写入到新的队列； 入队459460461462463464465466467468469470final void pushTask(ForkJoinTask&lt;?&gt; t) { ForkJoinTask&lt;?&gt;[] q; int s, m; if ((q = queue) != null) { // ignore if queue removed long u = (((s = queueTop) &amp; (m = q.length - 1)) &lt;&lt; ASHIFT) + ABASE; UNSAFE.putOrderedObject(q, u, t); queueTop = s + 1; // or use putOrderedInt if ((s -= queueBase) &lt;= 2) pool.signalWork(); else if (s == m) growQueue(); }} 如果队列中的任务数大于2，则通知线程池唤醒或者创建一个worker线程；如果队列已经满了（s == m），则通过growQueue对队列进行扩容。 出队出队分两种，一种从队列头部出队（当前worker线程），别一种从队列尾部出队（其它worker线程）。 从队列头部出队： 546547548549550551552553554555556557558559560561562563private ForkJoinTask&lt;?&gt; popTask() { int m; ForkJoinTask&lt;?&gt;[] q = queue; if (q != null &amp;&amp; (m = q.length - 1) &gt;= 0) { for (int s; (s = queueTop) != queueBase;) { int i = m &amp; --s; long u = (i &lt;&lt; ASHIFT) + ABASE; // raw offset ForkJoinTask&lt;?&gt; t = q[i]; if (t == null) // lost to stealer break; if (UNSAFE.compareAndSwapObject(q, u, t, null)) { queueTop = s; // or putOrderedInt return t; } } } return null;} 主要逻辑如下： 在队列不为空的情况下，从queueTop - 1位置处读取元素； 如果元素不为null，则通过UNSAFE.compareAndSwapObject将queueBase对应的元素置为null； 如果上述CAS成功，将该元素返回，并将queueTop减1；如果CAS失败，则重试。 从队列尾部出队： 506507508509510511512513514515516517final ForkJoinTask&lt;?&gt; deqTask() { ForkJoinTask&lt;?&gt; t; ForkJoinTask&lt;?&gt;[] q; int b, i; if (queueTop != (b = queueBase) &amp;&amp; (q = queue) != null &amp;&amp; // must read q after b (i = (q.length - 1) &amp; b) &gt;= 0 &amp;&amp; (t = q[i]) != null &amp;&amp; queueBase == b &amp;&amp; UNSAFE.compareAndSwapObject(q, (i &lt;&lt; ASHIFT) + ABASE, t, null)) { queueBase = b + 1; return t; } return null;} 主要逻辑如下： 在队列不为空，并且queueBase对应位置的元素不为null，从queueBase读取元素； 通过UNSAFE.compareAndSwapObject将queueBase对应的元素置为null； 如果上述CAS成功，将queueBase位置对应的元素返回，并将queueBase加1。 提交任务ForkJoinPool提供了类似ThreadPoolExecutor的接口来提供普通任务或者ForkJoinTask，这些接口最终都会调用forkOrSubmit来完成任务提交： 15291530153115321533153415351536153715381539private &lt;T&gt; void forkOrSubmit(ForkJoinTask&lt;T&gt; task) { ForkJoinWorkerThread w; Thread t = Thread.currentThread(); if (shutdown) throw new RejectedExecutionException(); if ((t instanceof ForkJoinWorkerThread) &amp;&amp; (w = (ForkJoinWorkerThread)t).pool == this) w.pushTask(task); else addSubmission(task);} 可以看到，forkOrSubmit要么将任务提交到对应worker线程的任务队列（提交任务的线程本身就是worker线程，并且该worker线程属于当前ForkJoinPool，通过w.pushTask提交任务，前文已分析过），要么将任务提交到ForkJoinPool提供的任务队列。 看一下addSubmission的实现： 15291530153115321533153415351536153715381539154015411542154315441545private void addSubmission(ForkJoinTask&lt;?&gt; t) { final ReentrantLock lock = this.submissionLock; lock.lock(); try { ForkJoinTask&lt;?&gt;[] q; int s, m; if ((q = submissionQueue) != null) { // ignore if queue removed long u = (((s = queueTop) &amp; (m = q.length-1)) &lt;&lt; ASHIFT)+ABASE; UNSAFE.putOrderedObject(q, u, t); queueTop = s + 1; if (s - queueBase == m) growSubmissionQueue(); } } finally { lock.unlock(); } signalWork();} 基本逻辑跟pushTask一致，只不过多加了个锁（同一时间，可能会有多个外部线程提交任务），并且是每加一个任务就会调用singalWork。 fork子任务也就是当前任务fork一个子任务，看一下实现： 621622623624625public final ForkJoinTask&lt;V&gt; fork() { ((ForkJoinWorkerThread) Thread.currentThread()) .pushTask(this); return this;} 比较简单，就是将任务提交到当前worker线程的任务队列。 join子任务等待子任务的完成： 638639640641642643public final V join() { if (doJoin() != NORMAL) return reportResult(); else return getRawResult();} 348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377private int doJoin() { Thread t; ForkJoinWorkerThread w; int s; boolean completed; if ((t = Thread.currentThread()) instanceof ForkJoinWorkerThread) { if ((s = status) &lt; 0) return s; if ((w = (ForkJoinWorkerThread)t).unpushTask(this)) { /** * unpushTask与上面分析的popTask实现类似，只是多了个判断，队列头的任务是不是当前任务。 * 也就是说，当join任务时，如果当前任务就在队列头部，就直接在当前worker线程执行。 */ try { completed = exec(); } catch (Throwable rex) { return setExceptionalCompletion(rex); } if (completed) return setCompletion(NORMAL); } /** * 任务不在队列头部，调用joinTask等待任务完成。 */ return w.joinTask(this); } else /** * 不是worker线程，直接调用Object.wait等待任务完成。 */ return externalAwaitDone();} 我们来看一下joinTask的实现： 708709710711712713714715716717718719720721722723724725726727728729730731732733734final int joinTask(ForkJoinTask&lt;?&gt; joinMe) { ForkJoinTask&lt;?&gt; prevJoin = currentJoin; currentJoin = joinMe; for (int s, retries = MAX_HELP;;) { if ((s = joinMe.status) &lt; 0) { currentJoin = prevJoin; return s; } if (retries &gt; 0) { if (queueTop != queueBase) { if (!localHelpJoinTask(joinMe)) retries = 0; // cannot help } else if (retries == MAX_HELP &gt;&gt;&gt; 1) { --retries; // check uncommon case if (tryDeqAndExec(joinMe) &gt;= 0) Thread.yield(); // for politeness } else retries = helpJoinTask(joinMe) ? MAX_HELP : retries - 1; } else { retries = MAX_HELP; // restart if not done pool.tryAwaitJoin(joinMe); } }} 主要流程： localHelpJoinTask：如果当前工作线程的任务队列不为空，则尝试在当前线程执行一个任务（未必是要join的任务）；但是如果任务队列的头部已经有一个任务在等待任务完成，则通过Object.wait等待任务完成； tryDeqAndExec：如果要join的任务在某个工作线程任务队列的尾部，则直接把任务偷取过来并执行； helpJoinTask：找到偷取当前任务的工作线程，并从其队列尾部偷取一个任务执行；如果该工作线程也在等待一个任务完成，则继续递归寻找偷取该任务的工作线程。 偷取任务偷取任务的逻辑很简单，就是从其它工作线程的队列尾部（queueBase）出队一个任务，并在当前工作线程中执行。可以看一下helpJoinTask中的一段代码： 806807808809810811812813814815if (t != null &amp;&amp; v.queueBase == b &amp;&amp; UNSAFE.compareAndSwapObject(q, u, t, null)) { // 获取到队列尾部的任务，通过CAS将队列中对应位置设为null v.queueBase = b + 1; // 更新queueBase v.stealHint = poolIndex; // 将stealHint设为当前工作线程 ForkJoinTask&lt;?&gt; ps = currentSteal; currentSteal = t; t.doExec(); // 在当前工作线程中执行偷取到的任务 currentSteal = ps; helped = true;}","link":"/2013/09/17/java-concurrent-source-code-reading-3/"},{"title":"队列系统问题总结","text":"概述对队列系统至今出现的各种问题进行总结。这个系统主要是分为这么几个部分： RabbitMQ：消息broker； Proxy：架在RabbitMQ前面，主要作用是负载均衡及高可用：消息可以路由到后端多个结点，任一结点的异常不会影响客户端；并且可以让RabbitMQ更方便的进行水平扩展； 客户端SDK：为了避免让产品方了解AMQP协议的细节（Exchange、bindings等），对标准的RabbitMQ客户端进行封装，只提供两个简单的接口：sendMessage，consumeMessage，并提供配置选项来定制客户端的行为。 Proxy无法接入TCP连接（代码问题）现象客户端无法建立TCP连接，查看TCP状态发现：客户端TCP连接处于ESTABLISHED状态，但服务端TCP连接处于SYC_RECV状态。抓包发送服务端在三次握手的第二步向客户端发送了SYN+ACK后没有接受客户端的ACK数据，如下图： 客户端认为三步握手已经完成，但是服务端却一直在重传握手第二步的数据，导致客户端一直在重传握手正常完成后应该发送的第一个数据包（AMQP的协议头）。 原因一开始不太清楚为什么TCP会处于这种状态，后经大牛提醒：服务端在未accept的情况下处于SYC_RECV状态，如下图所示： (注：图片引用自@淘叔度微博，accept应该是在要SYN_RECV状态之后发生，而不是之前) 知道这个原因的前提下，查看代码（网络部分代码使用的是RabbitMQ的代码），会发现类似下面的代码： 123gen_event:which_handlers(error_logger),prim_inet:async_accept(LSock, -1),... 通过Erlang的Remote Shell进入Proxy进程查看，发现代码果然阻塞在gen_event:which_handlers/1这行。看注释这行的目的主要是为了清空日志进程的信箱，如果在特定环境（如内网）下可以不用。实现上可以简化为一个进程向另一个进程发送一条消息，然后等待响应，然后怀疑目标进程挂了，但是重试后发现目标进程正常。。。（上述进程都是指Erlang进程） 怀疑RabbitMQ是不是也会出现类似的问题，但是跑了一段时间的测试，发现RabbitMQ本身并没有出现这个问题。而Proxy与RabbitMQ在这块不一样的是使用了一个Erlang的日志框架lager，难道跟这个有关系？去除lager依赖，再跑测试，问题不再出现。 解决方案当前的解决方案是去除上面这句代码：gen_event:which_handlers/1，同时向lager的官方社区提了issue。 客户端SDK死锁（代码问题）现象在一次更新后，发现使用SDK的Tomcat进程在一段时间后会出现线程数激增，客户端无响应。把Thread状态dump出来以后，看到大量线程在等锁： 1234java.lang.Thread.State: BLOCKED (on object monitor) at com.netease.mq.client.AbstractSimpleClient.getChannel(AbstractSimpleClient.java:311) - waiting to lock &lt;0x000000078b656bd8&gt; (a com.netease.mq.client.producer.SimpleMessageProducer) at com.netease.mq.client.producer.SimpleMessageProducer.sendMessage(SimpleMessageProducer.java:78) 而持有锁的进程在等待Proxy的响应： 1234567891011121314151617java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) at java.lang.Object.wait(Object.java:485) at com.rabbitmq.utility.BlockingCell.get(BlockingCell.java:50) - locked &lt;0x00000007866a31c8&gt; (a com.rabbitmq.utility.BlockingValueOrException) at com.rabbitmq.utility.BlockingCell.uninterruptibleGet(BlockingCell.java:89) - locked &lt;0x00000007866a31c8&gt; (a com.rabbitmq.utility.BlockingValueOrException) at com.rabbitmq.utility.BlockingValueOrException.uninterruptibleGetValue(BlockingValueOrException.java:33) at com.rabbitmq.client.impl.AMQChannel$BlockingRpcContinuation.getReply(AMQChannel.java:343) at com.rabbitmq.client.impl.AMQChannel.privateRpc(AMQChannel.java:216) at com.rabbitmq.client.impl.AMQChannel.exnWrappingRpc(AMQChannel.java:118) at com.rabbitmq.client.impl.ChannelN.confirmSelect(ChannelN.java:1052) at com.rabbitmq.client.impl.ChannelN.confirmSelect(ChannelN.java:61) at com.netease.mq.client.AbstractSimpleClient.createChannel(AbstractSimpleClient.java:342) at com.netease.mq.client.AbstractSimpleClient.getChannel(AbstractSimpleClient.java:323) - locked &lt;0x000000078b656bd8&gt; (a com.netease.mq.client.producer.SimpleMessageProducer) at com.netease.mq.client.producer.SimpleMessageProducer.sendMessage(SimpleMessageProducer.java:78) 看到这个堆栈的第一反应是Proxy出问题了，但是查看同一时间Proxy的日志显示，在路由消息（createChannel）到后端的时候发生了超时，并关闭了客户端连接。但是客户端竟然没有抛出异常，诡异。 原因无意之间发现一个处于Waiting状态的线程，也在等待Proxy的响应： 12345678910111213141516171819202122java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) at java.lang.Object.wait(Object.java:485) at com.rabbitmq.utility.BlockingCell.get(BlockingCell.java:50) - locked &lt;0x000000078669fc98&gt; (a com.rabbitmq.utility.BlockingValueOrException) at com.rabbitmq.utility.BlockingCell.get(BlockingCell.java:65) - locked &lt;0x000000078669fc98&gt; (a com.rabbitmq.utility.BlockingValueOrException) at com.rabbitmq.utility.BlockingCell.uninterruptibleGet(BlockingCell.java:111) - locked &lt;0x000000078669fc98&gt; (a com.rabbitmq.utility.BlockingValueOrException) at com.rabbitmq.utility.BlockingValueOrException.uninterruptibleGetValue(BlockingValueOrException.java:37) at com.rabbitmq.client.impl.AMQChannel$BlockingRpcContinuation.getReply(AMQChannel.java:349) at com.rabbitmq.client.impl.ChannelN.close(ChannelN.java:567) at com.rabbitmq.client.impl.ChannelN.close(ChannelN.java:499) at com.rabbitmq.client.impl.ChannelN.close(ChannelN.java:492) at com.netease.mq.client.AbstractSimpleClient$1.onRemoval(AbstractSimpleClient.java:255) at com.google.common.cache.LocalCache.processPendingNotifications(LocalCache.java:2016) at com.google.common.cache.LocalCache$Segment.runUnlockedCleanup(LocalCache.java:3521) at com.google.common.cache.LocalCache$Segment.postWriteCleanup(LocalCache.java:3497) at com.google.common.cache.LocalCache$Segment.remove(LocalCache.java:3168) at com.google.common.cache.LocalCache.remove(LocalCache.java:4236) at com.google.common.cache.LocalCache$LocalManualCache.invalidate(LocalCache.java:4815) at com.netease.mq.client.AbstractSimpleClient$2.shutdownCompleted(AbstractSimpleClient.java:352) 看到这个线程的堆栈，可以确定Proxy关闭连接的事件客户端SDK已经捕捉到，并且触发了相关处理逻辑（AbstractSimpleClient$2.shutdownCompleted）。后面的逻辑就是导致死锁的更新的主要内容：根据需要回收已经过期的channel。这时候，客户端SDK会向Proxy发送一个channel.close命令，然后等待响应，但是连接已经关闭了，所以永远不可能等到响应。问题是： 这时候，需要回收的channel不知道连接已关闭？ 就算不知道，在已关闭的连接上发送数据不会抛出异常？ 为了重现这个现象，修改Proxy的代码，channel数量到一定水平，新打开channel时产生与上述问题一致的行为：等待会使channel过期的时间后关闭连接，可以稳定重现死锁。然后回答下上面的两个问题： 触发AbstractSimpleClient$2.shutdownCompleted逻辑的channel确实知道连接已经关闭，并且是第一个知道连接已经关闭的channel，其它的channel会依次得到通知；但是在第一个channel触发回收时，其它channel是不知道连接已经关闭； 经过测试，服务端已经关闭的情况下，客户端在此连接上发送数据不会触发异常，参考这里及这里。 解决方案在回收Channel时，如果连接已经关闭，则不再发送关闭请求，直接跳过。 其它问题AMQP qos（设计问题）因为Proxy的存在，后端多个结点在客户端看来像一个结点，但是basic.qos这条命令会发送到所有后端几点，这样导致客户端本来期望收到1条消息，但是实际会收到多条消息。这个导致在使用nodejs的AMQP客户端的会出现问题（nodejs客户端提供了一个不带参数的ack方法，只会ack最后一次收到的消息，可应用依赖于只收到一条消息，当收到多条消息时，就会将一条消息ack多次）。 但是这个问题要处理得当也比较麻烦，需要考虑各种情况下的调整： 如果qos要求是1，但后端结点数量大于1，怎么处理？如果只发送到qos到一个结点，这个结点挂了，需要如何处理？ 如果qos比较大，可以平分到后端结点，那一个结点挂了，如何处理？调高其它结点的qos？那这个结点又恢复了，怎么处理？再把其它结点的qos调低？ 如果遇到扩容，缩容的需求怎么处理？ 从上面的分析可知，这个问题要处理得当，Proxy会有很复杂的逻辑，所以当前的处理是保持现状，应用的业务逻辑不应依赖于qos的变化。 connection reset（代码问题）客户端SDK在运行一段时间后，会出现connection reset，查看日志后发现Proxy保存的channel数据有异常：channel在关闭时没有清除Proxy内与该channel相关的数据，而客户端又一直在打开，关闭channel，但是channel最多只能开到65535个，超过这个数量后会重新从1开始，导致使用了脏数据。 RabbitMQ后端重启，Proxy重连后，无法下发数据（代码问题）本来Proxy的设计是在后端结点重启时，Proxy会重试连接。实际在更新时，却发现Proxy重连成功后，数据无法下发到客户端，但是抓包发送数据发送到Proxy。测试后发现是Proxy在做重连逻辑时，未清除某些状态，导致数据一直缓存在Proxy这一层。 basic.consume没有发送到HA模式下的所有结点（代码问题）通过RabbitMQ的管理页面看到，建立到后端的连接，只有一个会消费消息，其它连接都没在干事。一开始怀疑客户端未发送正确发送basic.consume命令，后测试发现是Proxy在HA模式下的时候，只会将某些命令（queue.declare，exchange.declare等）发送到一个结点，但basic.consume需要发送到所有结点。 [2013.11.14更新] Proxy直接Crash（代码问题）前一天晚上收到报警，两台机器的Proxy都挂了，上去服务器看了下，有erl_crash.dump文件，时间差不多，一个在00:10分，一个在00:12分，把dump文件拉到本地，用CrashDumpViewer查看，看到错误信息： no more index entries in atom_tab (max=1048576) 看样子像是创建了大量的atom，在CrashDumpViewer里看到大量ClientReader进程注册名的atom，但是没看到其它相应工作进程的注册名。后来反应到Proxy前面有HAProxy，是要做定期健康检查的，每次的检查都需要新建一个TCP连接，同时创建一个ClientReader进程，但是这个TCP连接会马上断开，也就是不会发送我们的期望的AMQP协议数据。看了下代码，进程注册名称的行为果然发生在进程一创建的瞬间，而不是检测到AMQP协议数据的时候，所以每次的健康检查都会导致多一个atom，前端两台HAProxy就是每次检查多出2个atom，长期运行，最终导致atom超过最大数量。 [2013.12.5更新] 11.14的更新有问题，当时以为前端是HAProxy，其实不是。上次出现问题后，我以为另外一个部署环境也会出现同样的问题，但是却一直没有看到，当时以为两个环境的HAProxy配置有差异，后来跟SA确认后，才知道：1）出问题的环境前端部署的是LVS，因为这个环境用的是物理机；2）没出问题的环境用的才是HAProxy，因为后端是虚拟机，暂时没法用LVS。 也就是说LVS跟HAProxy的健康检查的机制不一样？试着在两个部署环境的测试环境中加日志，只要有TCP连接就打一条日志，结果是：1）LVS的环境每隔8秒会出一条日志，也就是说后端会收到LVS的健康检查连接；2）HAProxy的环境一条日志也没，难道根本就没收到健康检查的连接？诡异。。。 用tcpdump抓包后，才知道原来HAProxy在做健康检查的时候，三次握手根本都没完成，在第三步本应该发送ACK的时候，HAProxy发送了一个RST包，所以应用层不可能检测到连接，如下图所示： 而LVS的健康检查是在三次握手完成后再发一个RST包来断开连接的，如下图所示： 那这样有什么好处，我觉得有两点： 少发一个数据包，连接少倒没什么，如果有大量连接，可能HAProxy会更高效点； 不完成三次连接，应用层就不会检测到连接，对应用的逻辑没有影响，有些应用可能在连接一建立就会做一些初始化工作或者打一些日志：我们的Proxy本来就是在连接一建立的就会打一条日志，后来就是因为在LVS环境下，大量的健康检查连接导致大量的日志，所以把日志打印推迟到收到实际的AMQP数据才打印。 从上面来看，HAProxy考虑的更周到。 [2013.12.12更新] 生产者Confirm超时（代码问题）观察到的现象时，生产者偶尔会出现等待服务端confirm超时的现象（一开始设置的超时时间5秒，后来设置到10秒，30秒），发现问题依旧。一开始并没有很重视这个问题，主要有两个原因：1）超时不会导致消息丢失，只会导致消息重复，在我们的应用场景中，重复没有影响（客户端最终会做去重处理）；2）一开始的测试表明，通过Proxy及直接通过RabbitMQ都会导致超时，所以怀疑跟RabbitMQ的分布式机制有关系。 最近这个问题基本上每天都会出现，而且了解RabbitMQ的分布式机制后，觉得这个不是超时产生的原因，所以决定重做测试。一开始测试客户端在本地，很Happy的是跑第一遍的时候出现超时，虽然说超时的时机不确定，抓包后发现有丢包，也就是发送的数据包根本没到服务端，所以服务端也不可能会confirm，跟同事分析结果的时候，有人提醒我们本地连服务器的VPN是走UDP的，所以丢包也正常。。。，也就是说以前的测试结论也不成立：有可能只是通过Proxy会出问题，而直接通过RabbitMQ不会出问题。 基于以上了解，开始把测试程序部署到线上的机器上，跑了两天后，发现测试程序没出现超时的现象（不管是通过Proxy还是直接通过RabbitMQ），而线上程序还是基本每天有一两次的超时。看来只能通过线上的程序来诊断问题，随写了如下的Btrace脚本： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283@BTrace public class ConfirmTimeout { @TLS static Throwable currentException; @OnMethod( clazz=&quot;java.util.concurrent.TimeoutException&quot;, method=&quot;&lt;init&gt;&quot; ) public static void onThrow(@Self TimeoutException self) { println(self); currentException = self; } @OnMethod( clazz=&quot;com.rabbitmq.client.impl.ChannelN&quot;, method=&quot;waitForConfirms&quot;, location=@Location(Kind.THROW) ) public static void onConfirm(@Self ChannelN ch) { if(currentException != null) { Field chNumField = field(&quot;com.rabbitmq.client.impl.AMQChannel&quot;, &quot;_channelNumber&quot;); Field connField = field(&quot;com.rabbitmq.client.impl.AMQChannel&quot;, &quot;_connection&quot;); Field nextSeqField = field(&quot;com.rabbitmq.client.impl.ChannelN&quot;, &quot;nextPublishSeqNo&quot;); int chNum = getInt(chNumField, ch); long nextSeq = getLong(nextSeqField, ch); Object conn = get(connField, ch); long threadId = threadId(currentThread()); String tmp = timestamp(&quot;yyyy-MM-dd HH:mm:ss.SSSZ&quot;); tmp = strcat(tmp, &quot; &quot;); tmp = strcat(tmp, str(conn)); tmp = strcat(tmp, &quot; exception=&gt; &quot;); tmp = strcat(tmp, str(threadId)); tmp = strcat(tmp, &quot; : &quot;); tmp = strcat(tmp, str(chNum)); tmp = strcat(tmp, &quot; -&gt; &quot;); tmp = strcat(tmp, str(nextSeq)); println(tmp); currentException = null; } } @OnMethod( clazz=&quot;com.rabbitmq.client.impl.ChannelN&quot;, method=&quot;close&quot; ) public static void onCloseChannel(@Self ChannelN ch) { Field chNumField = field(&quot;com.rabbitmq.client.impl.AMQChannel&quot;, &quot;_channelNumber&quot;); Field connField = field(&quot;com.rabbitmq.client.impl.AMQChannel&quot;, &quot;_connection&quot;); Field nextSeqField = field(&quot;com.rabbitmq.client.impl.ChannelN&quot;, &quot;nextPublishSeqNo&quot;); int chNum = getInt(chNumField, ch); long nextSeq = getLong(nextSeqField, ch); Object conn = get(connField, ch); long threadId = threadId(currentThread()); String tmp = timestamp(&quot;yyyy-MM-dd HH:mm:ss.SSSZ&quot;); tmp = strcat(tmp, &quot; &quot;); tmp = strcat(tmp, str(conn)); tmp = strcat(tmp, &quot; close=&gt; &quot;); tmp = strcat(tmp, str(threadId)); tmp = strcat(tmp, &quot; : &quot;); tmp = strcat(tmp, str(chNum)); tmp = strcat(tmp, &quot; -&gt; &quot;); tmp = strcat(tmp, str(nextSeq)); println(tmp); } @OnMethod( clazz=&quot;com.rabbitmq.client.impl.AMQConnection&quot;, method=&quot;createChannel&quot;, location=@Location(Kind.RETURN) ) public static void onCreateChannel(@Self AMQConnection conn, @Return ChannelN ch) { Field chNumField = field(&quot;com.rabbitmq.client.impl.AMQChannel&quot;, &quot;_channelNumber&quot;); int chNum = getInt(chNumField, ch); long threadId = threadId(currentThread()); String tmp = timestamp(&quot;yyyy-MM-dd HH:mm:ss.SSSZ&quot;); tmp = strcat(tmp, &quot; &quot;); tmp = strcat(tmp, str(conn)); tmp = strcat(tmp, &quot; create=&gt; &quot;); tmp = strcat(tmp, str(threadId)); tmp = strcat(tmp, &quot; : &quot;); tmp = strcat(tmp, str(chNum)); println(tmp); }} 主要需要搞清楚这么几个问题：1）发生超时问题的连接是哪一个？（本来需要打印出连接地址，发送Btrace不允许调用对象的toString，所以只能打印对象ID）；2）发生问题的线程是哪一个？3）发生问题的channel ID是哪一个？4）发送问题时客户端等待的confirm消息的seq是什么？ 跑了一两天后，得到类似下面的输出日志： 12342013-12-11 16:57:11.733+0800 com.rabbitmq.client.impl.AMQConnection@4b0a1577 create=&gt; 213 : 372013-12-11 17:58:29.578+0800 com.rabbitmq.client.impl.AMQConnection@4b0a1577 close=&gt; 41 : 37 -&gt; 32013-12-11 18:27:28.530+0800 com.rabbitmq.client.impl.AMQConnection@4b0a1577 create=&gt; 196 : 372013-12-11 18:27:58.540+0800 com.rabbitmq.client.impl.AMQConnection@4b0a1577 exception=&gt; 196 : 37 -&gt; 2 观察到的现象是只要出现类似上面的序列：1）create channel A；2）close channel A（close是客户端SDK的主动回收机制引起的）；3）create channel A；第三步创建的channel发送的第一次消息肯定超时。 当时虽然也想到可能是Proxy没有把老的channel数据清除掉，所以导致问题，但是因为这一块以前已经处理过，在channel关闭的时候Proxy会主动清除数据，所以觉得不可能是这里的问题。 所以只好根据这些日志，重现这个问题。我们的客户端SDK用google的guava库来做的回收，这个库的特点是：1）没有独立的线程来做回收；2）回收发生在数据读写时；3）回收是分段进行的，类似ConcurrentHashMap里的分段锁。这里有两个东西要注意：1）哪些channel的操作会触发目标channel的回收操作？2）每次读写都会触发回收操作么？ 其中第二个问题一开始想当然的以为每次读数据都会触发回收，后来发现写的测试程序无法重现错误序列，大概翻了下guava的代码，发现读操作只有积累到一定量的时候才会触发（目前阈值是64）。 测试程序搞定后，通过Proxy发送数据可以稳定重现问题，但是直接通过RabbitMQ发送数据没有问题，看来还是Proxy的问题。仔细查看代码后，发现类似这样的逻辑： 12345678process_frame(Frame, State#ch{channel = Channel}) -&gt; NewState = handle_method(Method, State); put({channel, Channel}); ....handle_method(#'channel.close', State#ch{channel = Channel}) -&gt; erase({channel, Channel}), State; 了解Erlang的应该知道，erase是清除进程数据，put是添加（或更新）进程数据，也就是说channel.close的时候确实清了数据，但是外部调用函数又把它加回去了。。。","link":"/2013/10/30/summary-of-problems-occur-in-quueue-proxy/"},{"title":"Java中日志文件的读取","text":"问题应用中有时候会有读取日志文件，并做近实时分析的需求（日志监控等）。但是使用类似Log4j的日志框架，日志文件可能会滚动：老的日志文件重命名成其它文件名（比如以日期为后缀），生成一个与老文件同名的新文件，这时候就需要读取日志文件的线程能够正确区分新老文件，并读取相应更新并且不会漏读数据。当然，这个问题的前提是：日志文件本身只会append，而不会在文件中间写入或者删除。本文主要分享下解决这个问题时碰到的一些问题及解决方案。 问题分析首先我们来看看日志更新这件事有哪些情况可能会出现： 文件没有更新； 文件有更新，并且没有新的文件生成； 文件没有更新，并且有新的文件生成； 文件有更新，并且有新的文件生成； 也就是我们要解决两个问题：1）检测文件是否有更新，如果有更新，则读取更新；2）检测文件是否有滚动，也就是是否有新文件生成。 检测更新检测更新有两种可能的方法： 通过文件更新时间：File.lastModified可以获取； 通过文件大小：File.length或者 FileChannel.size。 文件更新时间通过文件更新时间检测更新应该是最直观的方法，但是这个方法有一个小缺陷：更新时间的精度。根据Wiki上的介绍，ext3的更新时间精度是一秒，ext4本身可以返回纳秒级别的更新时间，但是在这两种文件系统中，Java返回的都是秒级别的更新数据。这会导致什么问题呢？如果在1秒内有多次更新，那么有可能无法准确的检测到更新，如： lastmodified: 1365590117000, size: 10597 lastmodified: 1365590117000, size: 10610 如上所示，是测试中的一个样例，文件大小已变，但是更新时间并没有变化，导致无法检测到更新。当然本身这个问题影响并不大，如果文件后续又有更新，前面的更新还是会读到的。 文件大小用文件大小来判断一个文件是否有更新应该是最准确：只要文件哪怕更新一个字节，也会立即检测到文件有更新。 File.length拿到的永远是当前路径对应文件的大小，如果日志文件滚动，那么它拿到的就是新的日志文件的大小； FileChannel.size则永远拿到当前channel对应物理文件的大小，即使文件滚动，老的文件被重命名，这个size拿到的还是老的文件大小。 文件滚动在Linux中，inode是一个文件的唯一标识，不管文件是否重命名过。但是在Java中，却不存在这样一个接口来获取inode。在这种情况下，我们怎么来判断是否有文件生成？ 通过文件大小及更新时间（忽略精度问题）：上面提到FileChannel.size可以拿到老文件的大小，如果在两次检测之间，这个大小并没有变化，并且文件更新时间有变化（通过File.lastModified获取到，拿到的可能是新文件的更新时间），则可以肯定有新文件生成； 通过Runtime.exec（或者ProcessBuilder），来调用shell命令“ls -i”来获取文件的inode； 通过JNI来获取文件的inode。 如果不用获取inode，就可以判断新文件是否生成，那应该是最好的了，可惜并不完美：上述第一种方案的问题是，如果老的文件有更新，并且有新的文件生成，则检测不到已经有新文件生成了。在这种情况下，如果新的文件在滚动之前又有更新，则不会丢失数据（跟lastModified的精度问题类似），否则有可能会丢失数据。在实际使用中，如果检测间隔在毫秒级，这种情况应该很少出现。 那通过Runtime.exec获取inode可以吗，毕竟不用写jni代码。但是实际的情况是：1）每次获取inode花费的时间平均在10毫秒以下，如果检测间隔在100毫秒，还可以接受；2）通过这种方式获取inode不稳定，测试中发现有时候获取一次inode的时间会接近1s；3）Runtime.exec是通过fork一个进程，在新进程中执行shell命令的，万一系统中已经无法创建进程，那就会阻塞我们的检测线程；4）测了一下性能，分别通过Runtime.exec及JNI获取1000次inode，JNI耗时2ms，Runtime.exec耗时8s多，JNI的方式比Runtime.exec快了4000倍。。。 实现上面把问题基本已经分析清楚，那处理逻辑就比较简单下： 如果当前文件的inode与上一轮文件的inode不同，则认为有新的文件生成：如果FileChannel.size比上一轮的大，则先读取老文件更新；读取新文件的内容； 如果当前文件的inode与上一轮文件的inode相同，则没有新文件生成，只需要通过FileChannel.size来判断老文件是否有更新，如果有更新则读取。 NativeLoader做为一个工具类，如果在使用的时候还要配置一些参数什么的，那么无疑会比较麻烦。最简单的用法应该就是把打包好的jar发布到maven，使用方只要写好依赖，直接使用就行。为此，在打包时，就预先将编译好的动态库打包进去，并且由lib本身来加载动态库。NativeLoader就是完成从jar中加载动态库的功能。 总结最终代码见：log-tailer，只支持Linux及MacOS。","link":"/2013/10/03/tailing-log-files-in-java/"}],"tags":[{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"elf","slug":"elf","link":"/tags/elf/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"concurrent","slug":"concurrent","link":"/tags/concurrent/"},{"name":"io","slug":"io","link":"/tags/io/"},{"name":"lock","slug":"lock","link":"/tags/lock/"},{"name":"rabbitmq","slug":"rabbitmq","link":"/tags/rabbitmq/"},{"name":"ha","slug":"ha","link":"/tags/ha/"},{"name":"storage","slug":"storage","link":"/tags/storage/"},{"name":"llm","slug":"llm","link":"/tags/llm/"},{"name":"slm","slug":"slm","link":"/tags/slm/"},{"name":"erlang","slug":"erlang","link":"/tags/erlang/"},{"name":"tcp","slug":"tcp","link":"/tags/tcp/"},{"name":"btrace","slug":"btrace","link":"/tags/btrace/"},{"name":"inode","slug":"inode","link":"/tags/inode/"}],"categories":[{"name":"System","slug":"System","link":"/categories/System/"},{"name":"Java","slug":"Java","link":"/categories/Java/"},{"name":"Middleware","slug":"Middleware","link":"/categories/Middleware/"},{"name":"AI","slug":"AI","link":"/categories/AI/"},{"name":"MQ","slug":"Middleware/MQ","link":"/categories/Middleware/MQ/"},{"name":"Tools","slug":"Tools","link":"/categories/Tools/"}],"pages":[]}